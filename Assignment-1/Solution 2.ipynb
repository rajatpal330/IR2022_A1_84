{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "importing colab to read the google drive files"
      ],
      "metadata": {
        "id": "deheYPoOhM2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlyuGdEWdo9a",
        "outputId": "2d2f99ff-5b53-42d1-eb5a-9238e9560305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing all the necessary libraries"
      ],
      "metadata": {
        "id": "p43Sf4AjhT5B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx6JxJNGenZ7",
        "outputId": "7b7e372c-23e4-45c0-b40f-3c060ed89086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to get the list of all files in the folder"
      ],
      "metadata": {
        "id": "jpQFy2mHhaPH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v-JbRIyicDQv"
      },
      "outputs": [],
      "source": [
        "list1=os.listdir(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to read the content of the file"
      ],
      "metadata": {
        "id": "0a1OMXShhi0V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0_TXr_mK8MlM"
      },
      "outputs": [],
      "source": [
        "file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+\"awespinh.sal\",\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "txt=file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "contains the code to preprocess the raw query text"
      ],
      "metadata": {
        "id": "hZ6Tzg45hm64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_stemming(my_Text):\n",
        "  ps = PorterStemmer()\n",
        "  idx = 0 \n",
        "  for word in my_Text:\n",
        "      my_Text[idx] = ps.stem(word)\n",
        "      idx = idx + 1"
      ],
      "metadata": {
        "id": "vefJCtgp2-IV"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "98YFXpswgJhu"
      },
      "outputs": [],
      "source": [
        "def process(txt):\n",
        "\n",
        "  #punctuation removal\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "  for ele in txt:\n",
        "    if ele in punc:\n",
        "        txt = txt.replace(ele, \" \")\n",
        "  \n",
        "  #lower the text\n",
        "  txt_lower=txt.lower()\n",
        "\n",
        "  #remove stopwords\n",
        "  txt_stopwords=remove_stopwords(txt_lower)\n",
        "\n",
        "  #tokenization\n",
        "  txt_tokenize=nltk.word_tokenize(txt_stopwords)\n",
        "\n",
        "  #remove blank spaces\n",
        "  for word in txt_tokenize:\n",
        "    word=word.strip() \n",
        "\n",
        "  #stemming\n",
        "    my_stemming(txt_tokenize)\n",
        "\n",
        "  #remove duplicates \n",
        "  #txt_tokenize=list(set(txt_tokenize))\n",
        "\n",
        "  return txt_tokenize "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to build positional index datastructure using python dictonaries"
      ],
      "metadata": {
        "id": "_ZjLj8qmhzVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9W7t2li77sG"
      },
      "outputs": [],
      "source": [
        "dict_index={};\n",
        "dict_positional_index={};\n",
        "dict_frequency={};\n",
        "index=0\n",
        "for doc in list1:\n",
        "  file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+doc,\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "  dict_index[index]=doc\n",
        "  txt=file.read()\n",
        "  tokens=process(txt)\n",
        "  pos=0\n",
        "  for token in tokens:\n",
        "    if token in dict_positional_index:\n",
        "      if index in dict_positional_index[token]:\n",
        "        dict_positional_index[token][index].append(pos)\n",
        "      else:\n",
        "        dict_positional_index[token][index]=[]\n",
        "        dict_positional_index[token][index].append(pos)\n",
        "      dict_frequency[token]=dict_frequency[token]+1\n",
        "    else:\n",
        "      dict_positional_index[token]={}\n",
        "      dict_positional_index[token][index]=[]\n",
        "      dict_positional_index[token][index].append(pos)\n",
        "      dict_frequency[token]=1\n",
        "    pos+=1\n",
        "  index+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the positional index datastructure to use it again when requires"
      ],
      "metadata": {
        "id": "MOE9Cd9qh-dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPoLsY3cnM1T",
        "outputId": "f74a163f-f73d-4655-aa24-904bd6edb038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10543215"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "filehandler = open(\"dict_postional_index_1.txt\", 'wt',errors=\"ignore\",encoding =\"utf-8\")\n",
        "data = str(dict_positional_index)\n",
        "filehandler.write(data) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the datastructure - containing no of documents in which a particular word present"
      ],
      "metadata": {
        "id": "tW3O3ydyiMnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2f4FHEyp7ng",
        "outputId": "30760135-a0dc-4acb-cb80-3830f2d56dbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1334866"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "filehandler = open(\"dict_frequency_1.txt\", 'wt',errors=\"ignore\",encoding =\"utf-8\")\n",
        "data = str(dict_frequency)\n",
        "filehandler.write(data) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the data structure - containing documents id's of each document"
      ],
      "metadata": {
        "id": "gd45oGsVigb3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2DU_CmzqJ7c",
        "outputId": "bbaec6a6-cdc4-48af-a00e-f857d85fecd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22204"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "filehandler = open(\"dict_index_1.txt\", 'wt',errors=\"ignore\",encoding =\"utf-8\")\n",
        "data = str(dict_index)\n",
        "filehandler.write(data) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the positional index datastructure to use it again\n",
        "& using AST library to get the syntax of data structure"
      ],
      "metadata": {
        "id": "gAxCSlO9iqzp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1vGsrm8vLRY"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "file_result = open(\"/content/dict_postional_index_1.txt\", 'r',errors=\"ignore\",encoding =\"utf-8\")\n",
        "result1=file_result.read()\n",
        "dic=ast.literal_eval(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the documents -index datastructure using AST library(ABSTRACT SYNTAX TREE)"
      ],
      "metadata": {
        "id": "c3anYmu8kpTX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "2hOLKdESPAks"
      },
      "outputs": [],
      "source": [
        "file_second = open(\"/content/dict_index_1.txt\", 'r',errors=\"ignore\",encoding =\"utf-8\")\n",
        "result=file_second.read()\n",
        "dic2=ast.literal_eval(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to return the documents in which the phrasal query present\n",
        "input-- query\n",
        "output -- documents id"
      ],
      "metadata": {
        "id": "typBpnA5jlFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDoc(str):\n",
        "  str=process(str)\n",
        "  if len(str)==0:\n",
        "    return [{}]\n",
        "  elif len(str)==1:\n",
        "    return dic[str[0]]\n",
        "  else:\n",
        "    listing_curr=list(dic[str[0]].keys())\n",
        "    for next in range(1,len(str)):\n",
        "      curr=next-1\n",
        "      global_documents=[]\n",
        "      listing_next=list(dic[str[next]].keys())\n",
        "      i=0\n",
        "      j=0\n",
        "      while(i<len(listing_curr) and j<len(listing_next)):\n",
        "        documents={}\n",
        "        if listing_curr[i]==listing_next[j] :\n",
        "          #if(curr==0):\n",
        "          list_A=dic[str[curr]][listing_curr[i]]\n",
        "          #else:\n",
        "            #list_A=global[listing_curr[i]]\n",
        "          list_B=dic[str[next]][listing_next[j]]\n",
        "          m=0\n",
        "          n=0\n",
        "          position=[]\n",
        "          while(m<len(list_A) and n<len(list_B)):\n",
        "            if(list_A[m]+1==list_B[n]):\n",
        "              position.append(list_B[n])\n",
        "              m+=1\n",
        "              n+=1\n",
        "            elif list_A[m]<list_B[n]:\n",
        "              m+=1\n",
        "            else:\n",
        "              n+=1\n",
        "          if(len(position)>0):\n",
        "            documents[listing_next[j]]=position\n",
        "            global_documents.append(documents)\n",
        "          i+=1\n",
        "          j+=1\n",
        "        elif listing_curr[i]<listing_next[j] :\n",
        "          i+=1\n",
        "        else:\n",
        "          j+=1\n",
        "      if(len(documents)>0):\n",
        "        listing_curr=list(documents.keys())\n",
        "      else:\n",
        "        break\n",
        "    return global_documents\n"
      ],
      "metadata": {
        "id": "8Labf6JoQCuR"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to get the documents name from the documents id"
      ],
      "metadata": {
        "id": "Oum42yW7j3_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDocName(global_documents):\n",
        "  lis=[]\n",
        "  for i in range(0,len(global_documents)):\n",
        "    lis.append(list(global_documents[i].keys())[0])\n",
        "  doc_name=[]\n",
        "  for j in lis:\n",
        "    doc_name.append(dic2[j])\n",
        "  return doc_name\n"
      ],
      "metadata": {
        "id": "KZsIodAHbKAK"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we take query as user input and print -\n",
        "1) Number of documents in which the query presents \n",
        "2) Name of the documents in which the query presents"
      ],
      "metadata": {
        "id": "q43z2tzLkAWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str=input(\"enter query to search\")\n",
        "docs=getDoc(str)\n",
        "print(len(docs))\n",
        "names=getDocName(docs)\n",
        "print(names)"
      ],
      "metadata": {
        "id": "qroI6rVWerOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IR_assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}