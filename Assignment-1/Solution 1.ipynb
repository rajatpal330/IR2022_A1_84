{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOl7goH40qtx",
        "outputId": "77d4c3b0-7f66-4433-ab8d-041ac58f8b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG2Vueqo9X9z",
        "outputId": "ccd31af5-fbfa-4cd6-913c-5767006de88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uDRMw7idS-n"
      },
      "source": [
        "# **Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YyuujJbg9vQh"
      },
      "outputs": [],
      "source": [
        "file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+\"brownie.rec\",\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "txt=file.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uqsdztlm-Cgp"
      },
      "outputs": [],
      "source": [
        "my_list1=os.listdir(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEUDj1QLDLWl",
        "outputId": "ae2a2eae-55ad-43d0-bd49-f223f2e1a1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studentb.txt\n"
          ]
        }
      ],
      "source": [
        "print(my_list1[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "30RC-cx7-F4x"
      },
      "outputs": [],
      "source": [
        "my_list1.sort()\n",
        "\n",
        "#the path of the 10 fiels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "34QFv6msDgNZ",
        "outputId": "b130304e-bdce-4897-f024-00814514734c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'quack26.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "my_list1[846]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYQWkcqxmdWE",
        "outputId": "387319a8-e615-4c95-a2e6-ed92c27d409c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(my_list1))\n",
        "my_ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZcfEqSJyozch"
      },
      "outputs": [],
      "source": [
        "\n",
        "def my_stemming(my_Text):\n",
        "  ps = PorterStemmer()\n",
        "  idx = 0 \n",
        "  for word in my_Text:\n",
        "      my_Text[idx] = ps.stem(word)\n",
        "      idx = idx + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tEjhuQnPnoZm"
      },
      "outputs": [],
      "source": [
        "def process(txt):\n",
        "\n",
        "  #punctuation removal\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "  for ele in txt:\n",
        "    if ele in punc:\n",
        "        txt = txt.replace(ele, \" \")\n",
        "  \n",
        "  #lower the text\n",
        "  txt_lower=txt.lower()\n",
        "\n",
        "  #remove stopwords\n",
        "  txt_stopwords=remove_stopwords(txt_lower)\n",
        "  \n",
        "  #tokenization\n",
        "  txt_tokenize=nltk.word_tokenize(txt_stopwords)\n",
        "\n",
        "\n",
        "  #stemming :\n",
        "  my_stemming(txt_tokenize)\n",
        "\n",
        "  #remove blank spaces\n",
        "  for word in txt_tokenize:\n",
        "    word=word.strip()  \n",
        "\n",
        "\n",
        "  #remove duplicates \n",
        "  #txt_tokenize =list(set(txt_tokenize))\n",
        "  txt_tokenize=list(dict.fromkeys(txt_tokenize))\n",
        "\n",
        "  return txt_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tdra5bI5wX4C"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzsbzI0Qst6T",
        "outputId": "5c97c066-e6e6-495d-d493-91f0a1af2018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chocol', 'orgasm', 'rosi', 's', 'bakeri', 'butter', 'fresh', 'cream', 'sugar', 'pack', 'hold', 'bar', 'bake', 'book', 'judi', 'rosenberg', '4', 'oz', 'unsweeten', '8', 'tbsp', '1', 'stick', 'unsalt', 'room', 'temperatur', 'cup', 'plu', '2', 'tsp', 'vanilla', 'extract', '3', 'larg', 'egg', 'purpos', 'flour', 'chop', 'walnut', 'preheat', 'oven', '325', 'lightli', 'greas', 'inch', 'squar', 'pan', 'veget', 'oil', 'melt', 'doubl', 'boiler', 'place', 'simmer', 'water', 'cool', 'mixtur', '5', 'minut', 'medium', 'size', 'mix', 'bowl', 'pour', 'electr', 'mixer', 'speed', 'blend', '25', 'second', 'scrape', 'rubber', 'spatula', 'add', 'low', 'time', 'addit', 'yolk', 'broken', 'dispers', '10', 'velveti', '15', '20', 'finish', 'hand', 'certain', 'stir', 'nut', '6', 'spread', 'batter', 'evenli', 'prepar', 'sprinkl', 'remain', '7', 'browni', 'center', 'rack', 'crust', 'form', 'tester', 'insert', 'come', 'moist', 'crumb', '35', 'rise', 'height', 'edg', 'remov', 'complet', 'evapor', 'milk', 'frost', 'blender', 'thicken', '50', 'sound', 'machin', 'chang', 'process', 'occur', 'surfac', 'allow', 'sit', 'hour', 'cut', 'make', '36', 'small']\n"
          ]
        }
      ],
      "source": [
        "result=process(txt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EwFSNS2IsvBF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing has been done for 1 file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eXseniB40eb2"
      },
      "outputs": [],
      "source": [
        "#let us make the logic for preprocessing for all files "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaFYRYee0fh6",
        "outputId": "a576a08b-8a7a-41fa-ba1f-c38c2ff12fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n"
          ]
        }
      ],
      "source": [
        "all_Terms = []  # list to store all the terms in all the documents\n",
        "\n",
        "print(len(my_list1))\n",
        "\n",
        "\n",
        "for idx in range(0,len(my_list1)) :\n",
        "  curr_File = my_list1[idx] # name of the current file \n",
        "  file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+curr_File,\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "  my_txt=file.read()\n",
        "  curr_file_Title = process(my_txt)\n",
        "  all_Terms.append(curr_file_Title)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7qW0oOfVCsQ",
        "outputId": "1b266042-cfda-4063-c71a-c4085be5caf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n"
          ]
        }
      ],
      "source": [
        "print(len(all_Terms))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zPXS9czYXNbA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2-90v3PdNPH"
      },
      "source": [
        "##**Unigram Inverted Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0Xeet8wMqn9F"
      },
      "outputs": [],
      "source": [
        "\n",
        "my_Inverted_Dic = {} # for the inverted indexing \n",
        "doc_names_Dic = {} # for the dictionary and names mapping \n",
        "freq_Logic_Dic = {} # for maintaining the frequency \n",
        "\n",
        "# main inverted dictionary to show the posting related to the term \n",
        "# writting the logic for the 1 document \n",
        "\n",
        "\n",
        "for curr_doc in range(0,1133):\n",
        "  for idx in range(0,len(all_Terms[curr_doc])):\n",
        "\n",
        "    \n",
        "    curr_Term = all_Terms[curr_doc][idx]  # this is the current term \n",
        "\n",
        "    if curr_Term in my_Inverted_Dic:\n",
        "      freq_Logic_Dic[curr_Term] = freq_Logic_Dic[curr_Term] + 1 # frequency is added \n",
        "      my_Inverted_Dic[curr_Term][\"found in these docs\"].append(curr_doc)\n",
        "    \n",
        "    \n",
        "    \n",
        "    else:\n",
        "      my_Inverted_Dic[curr_Term] = {} #new dictionary for this word \n",
        "      my_Inverted_Dic[curr_Term][\"found in these docs\"] = [] \n",
        "      my_Inverted_Dic[curr_Term][\"found in these docs\"].append(curr_doc)\n",
        "      freq_Logic_Dic[curr_Term] = 1 #initially  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "      \n",
        "      #existing for the first time and seen for the first time \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPlfwH6_Nz7",
        "outputId": "b1950bde-b9fd-4c00-83b4-cbae87ebb706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('herbalherb1st', {'found in these docs': [0]}), ('aidcalendulacomfreyremediessickmedicin', {'found in these docs': [0]})]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dict_items = my_Inverted_Dic.items()\n",
        "\n",
        "first_two = list(dict_items)[:2]\n",
        "print(first_two)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dIVWW-EBFdH",
        "outputId": "8b97018c-5ff6-45b5-d70d-4a6fffb4dd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'found in these docs': [0, 1, 2, 11, 18, 20, 23, 29, 36, 38, 40, 45, 46, 48, 52, 53, 57, 58, 60, 62, 65, 67, 69, 70, 74, 75, 85, 91, 94, 97, 106, 108, 109, 121, 123, 124, 127, 129, 130, 131, 133, 134, 138, 139, 140, 145, 146, 147, 149, 150, 152, 156, 159, 167, 168, 169, 174, 175, 178, 179, 181, 184, 186, 187, 214, 219, 223, 225, 226, 228, 229, 232, 233, 234, 236, 237, 238, 239, 240, 241, 246, 252, 256, 257, 268, 273, 282, 284, 288, 297, 300, 315, 316, 321, 324, 328, 329, 330, 331, 340, 343, 345, 347, 348, 349, 354, 361, 363, 364, 365, 366, 368, 370, 373, 377, 379, 380, 383, 385, 386, 391, 392, 393, 394, 398, 400, 401, 402, 405, 406, 408, 418, 429, 430, 431, 436, 439, 441, 443, 450, 453, 455, 457, 470, 479, 483, 488, 489, 495, 501, 504, 510, 511, 513, 517, 518, 519, 520, 531, 532, 533, 534, 537, 538, 541, 543, 546, 547, 548, 549, 552, 557, 565, 575, 579, 584, 594, 598, 601, 608, 610, 611, 622, 625, 630, 632, 635, 641, 642, 648, 649, 658, 662, 663, 668, 670, 672, 673, 674, 687, 701, 706, 707, 713, 715, 719, 724, 730, 734, 738, 744, 747, 748, 749, 752, 760, 762, 764, 767, 770, 773, 777, 780, 781, 784, 785, 789, 790, 791, 792, 803, 808, 809, 811, 812, 813, 814, 815, 817, 836, 842, 846, 847, 851, 855, 864, 866, 869, 870, 871, 872, 874, 875, 876, 877, 878, 881, 882, 892, 904, 911, 914, 923, 926, 927, 928, 930, 937, 941, 945, 947, 972, 976, 978, 981, 984, 985, 991, 995, 1010, 1014, 1015, 1017, 1018, 1024, 1025, 1030, 1032, 1034, 1035, 1037, 1040, 1044, 1045, 1046, 1053, 1059, 1063, 1065, 1070, 1072, 1073, 1078, 1087, 1090, 1091, 1097, 1101, 1102, 1104, 1105, 1109, 1115, 1119, 1120, 1128, 1131, 1132]}\n"
          ]
        }
      ],
      "source": [
        "print(my_Inverted_Dic['water'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBCMXVRcMmYz"
      },
      "source": [
        "# **Query Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXYrVf0Qe-4U"
      },
      "source": [
        "## function for processing user query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mYbnXZ9xcm_Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def userQuery():\n",
        "  query_Filtered =[]\n",
        "  user_Query = input('Input Query') # taking the user input \n",
        "  query_Filtered=process(user_Query) #pre-processing the data\n",
        "  return query_Filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoAviJs57I89"
      },
      "source": [
        "#Taking the operators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kGdung17izgo"
      },
      "outputs": [],
      "source": [
        "my_op = []\n",
        "def take_Operators():\n",
        "  ko = input('Input Operation Sequence')\n",
        "  ko = ko[2:-2]\n",
        "  my_op = ko.split(\", \")\n",
        "  return my_op \n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyoC2YQSI1Kp"
      },
      "source": [
        "# OR, NOT, OR NOT, AND NOT LOGIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lgVw15Fd7bRv"
      },
      "outputs": [],
      "source": [
        "def OR(li_1, li_2,check,comp): \n",
        "  no_Doc = 0  \n",
        "  f_li = []  # final list that will be made from it  \n",
        "  total = len(li_1) + len(li_2)\n",
        "  i1 = 0 # index of list 1\n",
        "  i2 = 0 # index of list 2\n",
        "  len_1  = len(li_1)\n",
        "  len_2  = len(li_2)\n",
        "  comp = 0 \n",
        "  while (i1 < len_1) and (i2 < len_2) :\n",
        "\n",
        "    if li_1[i1] != li_2[i2]:\n",
        "      if li_1[i1] < li_2[i2]:\n",
        "        f_li.append(li_1[i1])\n",
        "        i1 += 1 \n",
        "        comp += 1\n",
        "      else :\n",
        "        f_li.append(li_2[i2])\n",
        "        i2 += 1\n",
        "        comp += 1\n",
        "\n",
        "    else :\n",
        "      #two common elements \n",
        "      f_li.append(li_1[i1])\n",
        "      i1 += 1\n",
        "      i2 += 1\n",
        "      comp += 1\n",
        "\n",
        "\n",
        "  while (i1 < len_1):\n",
        "    f_li.append(li_1[i1])\n",
        "    i1 += 1\n",
        "\n",
        "  \n",
        "  while (i2 < len_2):\n",
        "    f_li.append(li_2[i2])\n",
        "    i2 += 1\n",
        "    \n",
        "  \n",
        "  names_li = [] \n",
        "  \n",
        "  \n",
        "  for i in f_li:\n",
        "    names_li.append(my_list1[i])\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "  if check == 1 :\n",
        "    \n",
        "    print(\"Minimum number of Total Comparisions : \" , comp)\n",
        "    print(\"The number of Document Reterived : \" ,len(f_li)) \n",
        "    print(\"The list of Documents Names Retrieved : \", names_li)\n",
        "  return f_li,comp ; \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0iSWvbFdRMSW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiNgwvyydTN2"
      },
      "source": [
        "# And Logic "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "92iGXqB5IJGk"
      },
      "outputs": [],
      "source": [
        "def And_Logic(li_1, li_2,check,comp):\n",
        "  \n",
        "  no_Doc = 0  \n",
        "  f_li = []  # final list that will be made from it  \n",
        "  i1 = 0 # index of list 1\n",
        "  i2 = 0 # index of list 2\n",
        "  len_1  = len(li_1)\n",
        "  len_2  = len(li_2)\n",
        "\n",
        "  while (i1 < len_1) and (i2 < len_2) :\n",
        "\n",
        "    if li_1[i1] != li_2[i2]:\n",
        "      if li_1[i1] < li_2[i2]:\n",
        "        i1 += 1 \n",
        "        comp += 1\n",
        "      else :\n",
        "        i2 += 1\n",
        "        comp += 1\n",
        "\n",
        "    else :\n",
        "      #two common elements \n",
        "      f_li.append(li_1[i1])\n",
        "      i1 += 1\n",
        "      i2 += 1\n",
        "      comp += 1\n",
        "\n",
        "    \n",
        "  \n",
        "  names_li = [] \n",
        "  for i in f_li:\n",
        "    names_li.append(my_list1[i])\n",
        "\n",
        "  \n",
        "  if check == 1 :\n",
        "      \n",
        "    print(\"Minimum number of Total Comparisions : \" , comp)\n",
        "    print(\"The number of Document Reterived : \" ,len(f_li)) \n",
        "    print(\"The list of Documents Names Retrieved : \", names_li)\n",
        "  return f_li,comp\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xMCxUeniyk3"
      },
      "source": [
        "# OR NOT LOGIC "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mmUiwhpPi2dN"
      },
      "outputs": [],
      "source": [
        "def OR_NOT_Logic(li_1, li_2,check,comp):\n",
        "  # first we will do the not of li_2\n",
        "  i1 = 0 \n",
        "  i2 = 0 \n",
        "  arr_List = [] \n",
        "  for i in range (0,1133):\n",
        "    if i not in  li_2:\n",
        "      arr_List.append(i)\n",
        "  f_li = [] \n",
        "  f_li,comp = OR(li_1,arr_List,check,comp)\n",
        "  return f_li,comp\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drCSLpVSmC84"
      },
      "source": [
        "# AND NOT LOGIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QbEQ3IMNmFJT"
      },
      "outputs": [],
      "source": [
        "def AND_NOT_Logic(li_1, li_2,check,comp):\n",
        "  # first we will do the not of li_2\n",
        "  i1 = 0 \n",
        "  i2 = 0 \n",
        "  arr_List = [] \n",
        "  for i in range (0,1133):\n",
        "    if i not in  li_2:\n",
        "      arr_List.append(i)\n",
        "  \n",
        "  f_li = []\n",
        "  f_li,comp = And_Logic(li_1,arr_List,check,comp)\n",
        "  return f_li,comp\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oxvnPokhmXvp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BXZBjLUgmfNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2d6a82-c502-4074-d614-d8115bf6d596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write the number of Queries you want to perform !1\n",
            "Input Querylion stood thoughtfully for a moment\n",
            "Input Operation Sequence[ OR, OR, OR ]\n",
            "Minimum number of Total Comparisions :  409\n",
            "The number of Document Reterived :  411\n",
            "The list of Documents Names Retrieved :  ['a_fish_c.apo', 'a_tv_t-p.com', 'abbott.txt', 'acne1.txt', 'adameve.hum', 'aeonint.txt', 'age.txt', 'aids.txt', 'airlines', 'alflog.txt', 'all_grai', 'allfam.epi', 'allusion', 'ambrose.bie', 'anim_lif.txt', 'anime.lif', 'annoy.fascist', 'answers', 'appetiz.rcp', 'apsnet.txt', 'arab.dic', 'argotdic.txt', 'art-fart.hum', 'arthriti.txt', 'atherosc.txt', 'att.txt', 'b-2.jok', 'b12.txt', 'badday.hum', 'bank.rob', 'barney.txt', 'basehead.txt', 'bbh_intv.txt', 'beapimp.hum', 'beauty.tm', 'beer.gam', 'beesherb.txt', 'bhb.ill', 'billcat.hum', 'bingbong.hum', 'bitnet.txt', 'blackadd', 'blackhol.hum', 'blake7.lis', 'blooprs1.asc', 'bmdn01.txt', 'bnb_quot.txt', 'bnbeg2.4.txt', 'bnbguide.txt', 'boneles2.txt', 'booze1.fun', 'bored.txt', 'bozo_tv.leg', 'bread.rcp', 'butcher.txt', 'butwrong.hum', 'bw-phwan.hat', 'bw.txt', 'c0dez.txt', 'cabbage.txt', 'caesardr.sal', 'calculus.txt', 'candy.txt', 'cartoon.law', 'cartoon.laws', 'cartoon_.txt', 'cform2.txt', 'chickenheadbbs.txt', 'childhoo.jok', 'choco-ch.ips', 'christop.int', 'chung.iv', 'clancy.txt', 'classicm.hum', 'climbing.let', 'cmu.share', 'coffee.faq', 'cogdis.txt', 'collected_quotes.txt', 'college.txt', 'commutin.jok', 'computer.txt', 'conan.txt', 'consp.txt', 'cookie.1', 'cops.txt', 'cowexplo.hum', 'coyote.txt', 'cuchy.hum', 'cybrtrsh.txt', 'd-ned.hum', 'dalive', 'dead2.txt', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'deep.txt', 'devils.jok', 'dingding.hum', 'doc-says.txt', 'docspeak.txt', 'doggun.sto', 'drinks.gui', 'dromes.txt', 'drugshum.hum', 'drunk.txt', 'dthought.txt', 'eandb.drx', 'earp', 'econridl.fun', 'engineer.hum', 'english', 'english.txt', 'enlightenment.txt', 'epi_.txt', 'epi_bnb.txt', 'epi_rns.txt', 'epi_tton.txt', 'epikarat.txt', 'episimp2.txt', 'epitaph', 'eskimo.nel', 'exam.50', 'excuses.txt', 'exylic.txt', 'facedeth.txt', 'farsi.phrase', 'farsi.txt', 'fascist.txt', 'female.jok', 'fiber.txt', 'filmgoof.txt', 'films_gl.txt', 'final-ex.txt', 'finalexm.hum', 'fireplacein.txt', 'firstaid.txt', 'flux_fix.txt', 'food', 'fuckyou2.txt', 'fusion.gal', 'gas.txt', 'gd_gal.txt', 'gd_hhead.txt', 'gd_ql.txt', 'gd_tznew.txt', 'get.drunk.cheap', 'ghostfun.hum', 'golnar.txt', 'gown.txt', 'grail.txt', 'grommet.hum', 'grospoem.txt', 'hack', 'hackingcracking.txt', 'hackmorality.txt', 'hammock.hum', 'happyhack.txt', 'hbo_spec.rev', 'headlnrs', 'hecomes.jok', 'herb!.hum', 'hitler.59', 'hitler.txt', 'homebrew.txt', 'homermmm.txt', 'hoonsrc.txt', 'hop.faq', 'hum2', 'humor9.txt', 'iced.tea', 'idaho.txt', 'idr2.txt', 'impurmat.hum', 'incarhel.hum', 'indgrdn.txt', 'initials.rid', 'inlaws1.txt', 'insanity.hum', 'insult.lst', 'insults1.txt', 'insuranc.sty', 'iremember', 'is_story.txt', 'ivan.hum', 'japantv.txt', 'jason.fun', 'jayjay.txt', 'jc-elvis.inf', 'jokes', 'jokes.txt', 'jokes1.txt', 'kaboom.hum', 'kanalx.txt', 'kloo.txt', 'lampoon.jok', 'lansing.txt', 'laws.txt', 'lawyer.jok', 'lbinter.hum', 'let.go', 'letgosh.txt', 'letter.txt', 'libraway.txt', 'liceprof.sty', 'lif&love.hum', 'lifeimag.hum', 'lifeonledge.txt', 'limerick.jok', 'lines.jok', 'lion.jok', 'lion.txt', 'lions.cat', 'lipkovits.txt', 'llamas.txt', 'lll.hum', 'llong.hum', 'lotsa.jok', 'lozerzon.hum', 'lozeuser.hum', 'lucky.cha', 'luggage.hum', 'luvstory.txt', 'm0dzmen.hum', 'maecenas.hum', 'mailfrag.hum', 'manners.txt', 'marriage.hum', 'mash.hum', 'math.1', 'math.2', 'mcd.txt', 'mead.rcp', 'meinkamp.hum', 'mel.txt', 'melodram.hum', 'mensroom.jok', 'merry.txt', 'miami.hum', 'mindvox', 'minn.txt', 'misc.1', 'missdish', 'mlverb.hum', 'mog-history', 'montpyth.hum', 'moore.txt', 'moose.txt', 'moslem.txt', 'mrscienc.hum', 'mrsfield', 'msorrow', 'mundane.v2', 'murphy_l.txt', 'murphys.txt', 'mydaywss.hum', 'myheart.hum', 'naivewiz.hum', 'namaste.txt', 'nameisreo.txt', 'necropls.txt', 'news.hum', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.4', 'nigel.5', 'nigel.6', 'nigel.7', 'nigel10.txt', 'nihgel_8.9', 'nintendo.jok', 'novel.hum', 'nukewar.txt', 'number.killer', 'number_k.ill', 'nurds.hum', 'oam-001.txt', 'oculis.rcp', 'ohandre.hum', 'oldeng.hum', 'oldtime.txt', 'oliver.txt', 'oliver02.txt', 'onan.txt', 'onetoone.hum', 'onetotwo.hum', 'outawork.erl', 'oxymoron.jok', 'paddingurpapers.txt', 'parabl.hum', 'passage.hum', 'passenge.sim', 'peatchp.hum', 'pepper.txt', 'pepsideg.txt', 'petshop', 'phorse.hum', 'phxbbs-m.txt', 'pickup.txt', 'pizzawho.hum', 'planetzero.txt', 'policpig.hum', 'poll2res.hum', 'polly.txt', 'polly_.new', 'popmusi.hum', 'prac1.jok', 'prac2.jok', 'prac3.jok', 'prac4.jok', 'pracjoke.txt', 'practica.txt', 'pro-fact.hum', 'progrs.gph', 'psilaine.hum', 'psych_pr.quo', 'psycho.txt', 'pukeprom.jok', 'puzzles.jok', 'quack26.txt', 'quest.hum', 'quotes.jok', 'quotes.txt', 'quux_p.oem', 'radexposed.txt', 'radiolaf.hum', 'readme.bat', 'reasons.txt', 'reddwarf.sng', 'reeves.txt', 'relative.ada', 'rent-a_cat', 'rinaldo.jok', 'rinaldos.law', 'rinaldos.txt', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'roadpizz.txt', 'rocking.hum', 'rockmus.hum', 'scam.txt', 'scratchy.txt', 'sf-zine.pub', 'sfmovie.txt', 'shorties.jok', 'shuttleb.hum', 'signatur.jok', 'smackjok.hum', 'smartass.txt', 'smurfkil.hum', 'snapple.rum', 'socecon.hum', 'solders.hum', 'soleleer.hum', 'stone.hum', 'strine.txt', 'stuf10.txt', 'stuf11.txt', 'subrdead.hum', 'suicide2.txt', 'sungenu.hum', 'sw_err.txt', 'symbol.hum', 'taping.hum', 'tarot.txt', 'telecom.q', \"terrmcd'.hum\", 'terrnieg.hum', 'textgrap.hum', 'tfepisod.hum', 'thecube.hum', 'thievco.txt', 'three.txt', 'throwawa.hum', 'tickmoon.hum', 'timetr.hum', 'tnd.1', 'top10.txt', 'top10st1.txt', 'top10st2.txt', 'toxcwast.hum', 'tpquotes.txt', 'trekwes.hum', 'trukdeth.txt', 'twinkie.txt', 'twinpeak.txt', 'ukunderg.txt', 'urban.txt', 'valujet.txt', 'variety3.asc', 'various.txt', 'vegan.rcp', 'vonthomp', 'wacky.ani', 'wagon.hum', 'wedding.hum', 'who.txt', 'whoon1st.hum', 'whoops.hum', 'wimptest.txt', 'wisdom', 'wood', 'woodsmok.txt', 'woolly_m.amm', 'worldend.hum', 'wrdnws6.txt', 'xibovac.txt', 'yuban.txt', 'yuppies.hum']\n"
          ]
        }
      ],
      "source": [
        "#Generalized user input \n",
        "\n",
        "no_Que = int(input(\"Write the number of Queries you want to perform !\"))\n",
        "que = []\n",
        "ops = []  \n",
        "comp = 0 #number of comparisons \n",
        "for idx in range(0,no_Que):\n",
        "  que = userQuery()\n",
        "  ops=take_Operators()\n",
        "  uk = 0\n",
        "  curr_Res = []  #Result of the current execution \n",
        "\n",
        "  val = 1\n",
        "  save_Res = []\n",
        "  while(uk<len(ops)):\n",
        "    \n",
        "    check = 0 \n",
        "    my_op = ops[uk] # operator to be loaded \n",
        "    my_word1 = que[uk]\n",
        "    my_word2 = que[uk]\n",
        "  \n",
        "    li_2 = []\n",
        "    if uk==0 :\n",
        "      #when processing for the first time \n",
        "      if uk== len(ops)-1:\n",
        "        check =1 \n",
        "      li_1 = []\n",
        "      li_1 = my_Inverted_Dic[que[uk]]['found in these docs'] #first list \n",
        "      li_2 = my_Inverted_Dic[que[uk+1]]['found in these docs'] #second list\n",
        "      uk+=1  # updating \n",
        "      if my_op == 'OR':\n",
        "        #curr_Res,comp = OR(li_1,li_2,check,comp)\n",
        "        curr_Res,comp = OR(li_1,li_2,check,comp)\n",
        "\n",
        "      elif my_op == 'AND':\n",
        "        curr_Res,comp = And_Logic(li_1,li_2,check,comp)\n",
        "      elif  my_op == 'OR NOT':\n",
        "        curr_Res,comp = OR_NOT_Logic(li_1,li_2,check,comp)\n",
        "      elif  my_op == 'AND NOT':\n",
        "        curr_Res,comp = AND_NOT_Logic(li_1,li_2,check,comp)\n",
        "      else :\n",
        "        print(\"Wrong operator ! Please Check\")\n",
        "\n",
        "\n",
        "    else :\n",
        "\n",
        "      if uk==len(ops)-1:\n",
        "        check  = 1 \n",
        "\n",
        "      if uk == 1 : \n",
        "        save_Res = curr_Res\n",
        "      else :\n",
        "        save_Res = save_Res \n",
        "\n",
        "      #processing is not for the first time \n",
        "      li_2 = my_Inverted_Dic[que[uk+1]]['found in these docs'] #second list \n",
        "      uk += 1 #updating uk\n",
        "      if my_op == 'OR':\n",
        "        save_Res,comp = OR(save_Res,li_2,check,comp)\n",
        "      elif my_op == 'AND':\n",
        "        save_Res,comp = And_Logic(save_Res,li_2,check,comp)\n",
        "      elif  my_op == 'OR NOT':\n",
        "        save_Res,comp = OR_NOT_Logic(save_Res,li_2,check,comp)\n",
        "      elif  my_op == 'AND NOT':\n",
        "        save_Res,comp = AND_NOT_Logic(save_Res,li_2,check,comp)\n",
        "      else :\n",
        "        print(\"Wrong operator ! Please Check\")\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3ZAPxI3-np4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2472c4ae-4524-4fc8-c198-93f793122c4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AND NOT']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "ops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZwXcy0g9n7zB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OYTbL4ETn92L"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UDDZxYbK7-p5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment-1-IR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}