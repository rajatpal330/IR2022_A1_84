{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOl7goH40qtx",
        "outputId": "81e6d519-4cc0-4929-dd39-682d69a4d1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG2Vueqo9X9z",
        "outputId": "30a9b332-195e-46a8-df0f-973a2607ac70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "nltk.download(\"punkt\")\n",
        "import math\n",
        "import collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uDRMw7idS-n"
      },
      "source": [
        "# **Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEjhuQnPnoZm"
      },
      "outputs": [],
      "source": [
        "def process(txt):\n",
        "\n",
        "  #punctuation removal\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "  for ele in txt:\n",
        "    if ele in punc:\n",
        "        txt = txt.replace(ele, \" \")\n",
        "  \n",
        "  #lower the text\n",
        "  txt_lower=txt.lower()\n",
        "\n",
        "  #remove stopwords\n",
        "  txt_stopwords=remove_stopwords(txt_lower)\n",
        "  \n",
        "  #tokenization\n",
        "  txt_tokenize=nltk.word_tokenize(txt_stopwords)\n",
        "\n",
        "  #remove blank spaces\n",
        "  for word in txt_tokenize:\n",
        "    word=word.strip()  \n",
        "\n",
        "  #remove duplicates \n",
        "  txt_tokenize =set(txt_tokenize)\n",
        "  \n",
        "  return txt_tokenize "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKje4zVoYF2t"
      },
      "source": [
        "# jaccard similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPd45EqRXMya"
      },
      "source": [
        " **Code to generate document dictionary where ....Key -- document name\n",
        " value -- set of tokens in the document ** bold text** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEoTAO7bYKm6"
      },
      "outputs": [],
      "source": [
        "my_list1=os.listdir(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaFYRYee0fh6"
      },
      "outputs": [],
      "source": [
        "document_dict={}\n",
        "for idx in range(0,len(my_list1)) :\n",
        "  file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+my_list1[idx],\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "  my_txt=file.read()\n",
        "  token_list = process(my_txt)\n",
        "  document_dict[my_list1[idx]]=token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVeICDAEXsDS"
      },
      "source": [
        "**function to get jaccard coefficient of two sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3gYajhQ-uje"
      },
      "outputs": [],
      "source": [
        "def getJaccard(setA,setB):\n",
        "  return(len(setA.intersection(setB))/len(setA.union(setB)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrr-mHxiYguq"
      },
      "source": [
        "**creating dictionary where key is document name and value is the jaccard similarity between the query and document**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPXS9czYXNbA",
        "outputId": "e20d0e3d-f26d-4040-a401-178039e5e082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter the querysweetened\n"
          ]
        }
      ],
      "source": [
        "dict_jaccard={}\n",
        "str=input(\"enter the query\")\n",
        "str=process(str)\n",
        "for i in document_dict:\n",
        "  dict_jaccard[i]=getJaccard(str,document_dict[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK1IHWVsETd8"
      },
      "outputs": [],
      "source": [
        "import operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iefd4kbQYwQT"
      },
      "source": [
        "**reverse sorting the values to get relevent values at the top**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr273sHhA34E",
        "outputId": "7924fe19-ef28-4b54-db4c-9b1d27a13a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bread.rec', 0.010752688172043012), ('insect1.txt', 0.0053475935828877), ('booze1.fun', 0.003703703703703704), ('drinks.txt', 0.0034602076124567475), ('coffee.txt', 0.0024330900243309003), ('x-drinks.txt', 0.0014992503748125937), ('coffee.faq', 0.000397456279809221), ('candy.txt', 0.0002820078962210942), ('hell.jok', 0.0), ('hate.hum', 0.0), ('rinaldos.txt', 0.0), ('gohome.hum', 0.0), ('harmful.hum', 0.0), ('herb!.hum', 0.0), ('mothers.txt', 0.0), ('grommet.hum', 0.0), ('conan.txt', 0.0), ('coyote.txt', 0.0), ('murph.jok', 0.0), ('hi.tec', 0.0), ('icm.hum', 0.0), ('imprrisk.hum', 0.0), ('roach.asc', 0.0), ('howlong.hum', 0.0), ('interv.hum', 0.0), ('investi.hum', 0.0), ('bless.bc', 0.0), ('incarhel.hum', 0.0), ('insure.hum', 0.0), ('insanity.hum', 0.0), ('dirtword.txt', 0.0), ('horoscop.jok', 0.0), ('impurmat.hum', 0.0), ('cartoon_.txt', 0.0), ('bible.txt', 0.0), ('murphy.txt', 0.0), ('japrap.hum', 0.0), ('test.jok', 0.0), ('dym', 0.0), ('jac&tuu.hum', 0.0), ('ivan.hum', 0.0), ('killer.hum', 0.0), ('terms.hum', 0.0), ('testchri.txt', 0.0), ('kid_diet.txt', 0.0), ('test2.jok', 0.0), ('killself.hum', 0.0), ('kilsmur.hum', 0.0), ('motrbike.jok', 0.0), ('lbinter.hum', 0.0), ('lawsuniv.hum', 0.0), ('blooprs1.asc', 0.0), ('ludeinfo.txt', 0.0), ('simp.txt', 0.0), ('legal.hum', 0.0), ('murphys.txt', 0.0), ('ludeinfo.hum', 0.0), ('losers86.hum', 0.0), ('lifeinfo.hum', 0.0), ('lll.hum', 0.0), ('luggage.hum', 0.0), ('looser.hum', 0.0), ('livnware.hum', 0.0), ('lozerzon.hum', 0.0), ('lif&love.hum', 0.0), ('phunatdi.ana', 0.0), ('lozeuser.hum', 0.0), ('office.txt', 0.0), ('lobquad.hum', 0.0), ('catstory.txt', 0.0), ('losers84.hum', 0.0), ('lifeimag.hum', 0.0), ('llong.hum', 0.0), ('makebeer.hum', 0.0), ('manilla.hum', 0.0), ('meinkamp.hum', 0.0), ('m0dzmen.hum', 0.0), ('pun.txt', 0.0), ('mtm.hum', 0.0), ('maecenas.hum', 0.0), ('melodram.hum', 0.0), ('manspace.hum', 0.0), ('luzerzo2.hum', 0.0), ('mash.hum', 0.0), ('marines.hum', 0.0), ('madscrib.hum', 0.0), ('mailfrag.hum', 0.0), ('memo.hum', 0.0), ('montpyth.hum', 0.0), ('reasons.txt', 0.0), ('misery.hum', 0.0), ('calvin.txt', 0.0), ('mutate.hum', 0.0), ('mydaywss.hum', 0.0), ('miamadvi.hum', 0.0), ('mrscienc.hum', 0.0), ('miami.hum', 0.0), ('miranda.hum', 0.0), ('f_tang.txt', 0.0), ('missheav.hum', 0.0), ('newconst.hum', 0.0), ('nuke.hum', 0.0), ('naivewiz.hum', 0.0), ('nysucks.hum', 0.0), ('novel.hum', 0.0), ('boston.geog', 0.0), ('cartoon.law', 0.0), ('cars.txt', 0.0), ('myheart.hum', 0.0), ('cartwb.son', 0.0), ('nukeplay.hum', 0.0), ('nurds.hum', 0.0), ('news.hum', 0.0), ('newmex.hum', 0.0), ('o-ttalk.hum', 0.0), ('onetotwo.hum', 0.0), ('ozarks.hum', 0.0), ('oilgluts.hum', 0.0), ('odearakk.hum', 0.0), ('oldeng.hum', 0.0), ('p-law.hum', 0.0), ('opinion.hum', 0.0), ('chunnel.txt', 0.0), ('ookpik.hum', 0.0), ('deep.txt', 0.0), ('ohandre.hum', 0.0), ('onetoone.hum', 0.0), ('devils.jok', 0.0), ('planeget.hum', 0.0), ('peatchp.hum', 0.0), ('dieter.txt', 0.0), ('phorse.hum', 0.0), ('prayer.hum', 0.0), ('poll2res.hum', 0.0), ('popconc.hum', 0.0), ('policpig.hum', 0.0), ('pizzawho.hum', 0.0), ('parabl.hum', 0.0), ('prawblim.hum', 0.0), ('passage.hum', 0.0), ('popmusi.hum', 0.0), ('phony.hum', 0.0), ('donut.txt', 0.0), ('raven.hum', 0.0), ('ratspit.hum', 0.0), ('reagan.hum', 0.0), ('psilaine.hum', 0.0), ('dead-r', 0.0), ('radiolaf.hum', 0.0), ('quest.hum', 0.0), ('pro-fact.hum', 0.0), ('fuckyou2.txt', 0.0), ('ratings.hum', 0.0), ('rapmastr.hum', 0.0), ('rocking.hum', 0.0), ('report.hum', 0.0), ('reddye.hum', 0.0), ('drinkrul.jok', 0.0), ('docspeak.txt', 0.0), ('rockmus.hum', 0.0), ('research.hum', 0.0), ('rentals.hum', 0.0), ('repair.hum', 0.0), ('ripoffpc.hum', 0.0), ('insult', 0.0), ('drunk.txt', 0.0), ('corporat.txt', 0.0), ('t_zone.jok', 0.0), ('dubltalk.jok', 0.0), ('takenote.jok', 0.0), ('social.hum', 0.0), ('spydust.hum', 0.0), ('jokes', 0.0), ('socecon.hum', 0.0), ('solviets.hum', 0.0), ('smurfkil.hum', 0.0), ('smackjok.hum', 0.0), ('spider.hum', 0.0), ('iqtest', 0.0), ('cucumber.txt', 0.0), ('skippy.hum', 0.0), ('terbear.txt', 0.0), ('soleleer.hum', 0.0), ('teens.txt', 0.0), ('shuttleb.hum', 0.0), ('stone.hum', 0.0), ('taping.hum', 0.0), ('talebeat.hum', 0.0), ('y.txt', 0.0), ('t-10.hum', 0.0), ('standard.hum', 0.0), ('t-shirt.hum', 0.0), (\"terrmcd'.hum\", 0.0), ('sungenu.hum', 0.0), ('termpoem.txt', 0.0), ('texican.dic', 0.0), ('top10.txt', 0.0), ('televisi.hum', 0.0), ('languag.jok', 0.0), ('timetr.hum', 0.0), ('hecomes.jok', 0.0), ('thecube.hum', 0.0), ('textgrap.hum', 0.0), ('tfepisod.hum', 0.0), ('terrnieg.hum', 0.0), ('the_math.hel', 0.0), ('tfpoems.hum', 0.0), ('lawyer.jok', 0.0), ('tickmoon.hum', 0.0), ('toxcwast.hum', 0.0), ('let.go', 0.0), ('test.hum', 0.0), ('koans.txt', 0.0), ('cartoon.laws', 0.0), ('tribble.hum', 0.0), ('whoops.hum', 0.0), ('turbo.hum', 0.0), ('truths.hum', 0.0), ('wetdream.hum', 0.0), ('whoon1st.hum', 0.0), ('wedding.hum', 0.0), ('wagon.hum', 0.0), ('truthlsd.hum', 0.0), ('voltron.hum', 0.0), ('trekwes.hum', 0.0), ('why-me.hum', 0.0), ('twinkie.txt', 0.0), ('thesis.beh', 0.0), ('thermite.ana', 0.0), ('liceprof.sty', 0.0), ('word.hum', 0.0), ('yuppies.hum', 0.0), ('who.txt', 0.0), ('yjohncse.hum', 0.0), ('jeffie.heh', 0.0), ('zodiac.hum', 0.0), ('worldend.hum', 0.0), ('billcat.hum', 0.0), ('xtermin8.hum', 0.0), ('hotnnot.hum', 0.0), ('trukdeth.txt', 0.0), ('limerick.jok', 0.0), ('vaguemag.90s', 0.0), ('wagit.txt', 0.0), ('washroom.txt', 0.0), ('tshirts.jok', 0.0), ('univ.odd', 0.0), ('lines.jok', 0.0), ('urban.txt', 0.0), ('st_silic.txt', 0.0), ('waitress.txt', 0.0), ('units.mea', 0.0), ('math.2', 0.0), ('wimptest.txt', 0.0), ('lion.jok', 0.0), ('weight.txt', 0.0), ('misc.1', 0.0), ('marriage.hum', 0.0), ('woodbugs.txt', 0.0), ('welfare.txt', 0.0), ('strine.txt', 0.0), ('math.far', 0.0), ('yogisays.txt', 0.0), ('math.1', 0.0), ('lotsa.jok', 0.0), ('zgtoilet.txt', 0.0), ('climbing.let', 0.0), ('church.sto', 0.0), ('xibovac.txt', 0.0), ('chainltr.txt', 0.0), ('childhoo.jok', 0.0), ('lozers', 0.0), ('co-car.jok', 0.0), ('commutin.jok', 0.0), ('woolly_m.amm', 0.0), ('netnews.10', 0.0), ('hack', 0.0), ('element.jok', 0.0), ('nigel.3', 0.0), ('nigel.2', 0.0), ('electric.txt', 0.0), ('court.quips', 0.0), ('nigel.10', 0.0), ('eandb.drx', 0.0), ('elephant.fun', 0.0), ('fartinfo.txt', 0.0), ('nigel.6', 0.0), ('drinker.txt', 0.0), ('fascist.txt', 0.0), ('final-ex.txt', 0.0), ('idr2.txt', 0.0), ('jokes.txt', 0.0), ('ourfathr.txt', 0.0), ('nigel.5', 0.0), ('exam.50', 0.0), ('nigel.4', 0.0), ('fartting.txt', 0.0), ('excuses.txt', 0.0), ('lawskool.txt', 0.0), ('german.aut', 0.0), ('passenge.sim', 0.0), ('fusion.sup', 0.0), ('good.txt', 0.0), ('gd_guide.txt', 0.0), ('fuck!.txt', 0.0), ('grammar.jok', 0.0), ('grospoem.txt', 0.0), ('nigel.7', 0.0), ('rec.por', 0.0), ('lawhunt.txt', 0.0), ('gas.txt', 0.0), ('polemom.txt', 0.0), ('polly_.new', 0.0), ('poli_t.ics', 0.0), ('problem.txt', 0.0), ('dover.poem', 0.0), ('potty.txt', 0.0), ('moonshin', 0.0), ('pepsideg.txt', 0.0), ('polly.txt', 0.0), ('nintendo.jok', 0.0), ('proof.met', 0.0), ('pournell.spo', 0.0), ('pickup.lin', 0.0), ('pukeprom.jok', 0.0), ('prooftec.txt', 0.0), ('pure.mat', 0.0), ('psalm_re.aga', 0.0), ('proposal.jok', 0.0), ('leech.txt', 0.0), ('python_s.ong', 0.0), ('psych_pr.quo', 0.0), ('prover_w.iso', 0.0), ('curse.txt', 0.0), ('peanuts.txt', 0.0), ('college.hum', 0.0), ('psalm23.txt', 0.0), ('puzzles.jok', 0.0), ('quux_p.oem', 0.0), ('tuflife.txt', 0.0), ('eskimo.nel', 0.0), ('pickup.txt', 0.0), ('prac1.jok', 0.0), ('flowchrt.txt', 0.0), ('hitler.59', 0.0), ('pecker.txt', 0.0), ('smartass.txt', 0.0), ('hilbilly.wri', 0.0), ('height.txt', 0.0), ('pipespec.txt', 0.0), ('hitlerap.txt', 0.0), ('insuranc.sty', 0.0), ('htswfren.txt', 0.0), ('prac3.jok', 0.0), ('snipe.txt', 0.0), ('stereo.txt', 0.0), ('hoosier.txt', 0.0), ('televisi.txt', 0.0), ('humatra.txt', 0.0), ('steroid.txt', 0.0), ('inlaws1.txt', 0.0), ('idaho.txt', 0.0), ('law.sch', 0.0), ('humatran.jok', 0.0), ('smokers.txt', 0.0), ('horoscop.txt', 0.0), ('how_to_i.pro', 0.0), ('sysadmin.txt', 0.0), ('woodbine.txt', 0.0), ('letter.txt', 0.0), ('letter_f.sch', 0.0), ('laws.txt', 0.0), ('kloo.txt', 0.0), ('psycho.txt', 0.0), ('letgosh.txt', 0.0), ('sysman.txt', 0.0), ('jargon.phd', 0.0), ('lampoon.jok', 0.0), ('jokeju07.txt', 0.0), ('italoink.txt', 0.0), ('jc-elvis.inf', 0.0), ('rednecks.txt', 0.0), ('realest.txt', 0.0), ('primes.jok', 0.0), ('odd_to.obs', 0.0), ('lions.cat', 0.0), ('oxymoron.jok', 0.0), ('signatur.jok', 0.0), ('luvstory.txt', 0.0), ('oracle.jok', 0.0), ('smurfs.cc', 0.0), ('socks.drx', 0.0), ('prac4.jok', 0.0), ('nukwaste', 0.0), ('smiley.txt', 0.0), ('one.par', 0.0), ('farsi.phrase', 0.0), ('lucky.cha', 0.0), ('some_hu.mor', 0.0), ('spelin_r.ifo', 0.0), ('puzzle.spo', 0.0), ('jrrt.riddle', 0.0), ('studentb.txt', 0.0), ('startrek.txt', 0.0), ('strsdiet.txt', 0.0), ('humpty.dumpty', 0.0), ('telecom.q', 0.0), ('princess.brd', 0.0), ('mr.rogers', 0.0), ('rent-a_cat', 0.0), ('psalm_nixon', 0.0), ('nosuch_nasfic', 0.0), ('quick.jok', 0.0), ('un.happy', 0.0), ('prover.wisom', 0.0), ('poli.tics', 0.0), ('shrink.news', 0.0), ('psalm.reagan', 0.0), ('number.killer', 0.0), ('popmach', 0.0), ('resrch_phrase', 0.0), ('quantum.jok', 0.0), ('rinaldos.law', 0.0), ('quotes.txt', 0.0), ('top10.elf', 0.0), ('welfare', 0.0), ('progrs.gph', 0.0), ('squids.gph', 0.0), ('tnd.1', 0.0), ('temphell.jok', 0.0), ('racist.net', 0.0), ('quotes.jok', 0.0), ('texican.lex', 0.0), ('quotes.bug', 0.0), ('shorties.jok', 0.0), ('speling.msk', 0.0), ('aussie.lng', 0.0), ('bw-summe.hat', 0.0), ('bw-phwan.hat', 0.0), ('justify', 0.0), ('wisdom', 0.0), ('outawork.erl', 0.0), ('la_times.hun', 0.0), ('firstaid.inf', 0.0), ('supermar.rul', 0.0), ('ambrose.bie', 0.0), ('oasis', 0.0), ('oldtime.sng', 0.0), ('gaiahuma', 0.0), ('oxymoron.txt', 0.0), ('kid2', 0.0), ('anim_lif.txt', 0.0), ('bad', 0.0), ('wood', 0.0), ('talkbizr.txt', 0.0), ('woodsmok.txt', 0.0), ('bagelope.txt', 0.0), ('various.txt', 0.0), ('variety3.asc', 0.0), ('engrhyme.txt', 0.0), ('english', 0.0), ('variety2.asc', 0.0), ('empeval.txt', 0.0), ('epitaph', 0.0), ('variety1.asc', 0.0), ('excuse.txt', 0.0), ('excuse30.txt', 0.0), ('vegkill.txt', 0.0), ('quack26.txt', 0.0), ('godmonth.txt', 0.0), ('vonthomp', 0.0), ('q.pun', 0.0), ('cogdis.txt', 0.0), ('gown.txt', 0.0), ('growth.txt', 0.0), ('brewing', 0.0), ('hamburge.nam', 0.0), ('candybar.fun', 0.0), ('snapple.rum', 0.0), ('tuna.lab', 0.0), ('b12.txt', 0.0), ('cooking.fun', 0.0), ('mrsfield', 0.0), ('coladrik.txt', 0.0), ('jerky.rcp', 0.0), ('coladrik.fun', 0.0), ('bakebred.txt', 0.0), ('chili.txt', 0.0), ('beginn.ers', 0.0), ('coke.txt', 0.0), ('newcoke.txt', 0.0), ('meat2.txt', 0.0), ('mead.rcp', 0.0), ('vegan.rcp', 0.0), ('coke1', 0.0), ('oatbran.rec', 0.0), ('appetiz.rcp', 0.0), ('coke.fun', 0.0), ('curry.hrb', 0.0), ('kashrut.txt', 0.0), ('fajitas.rcp', 0.0), ('turkey.fun', 0.0), ('coke_fan.naz', 0.0), ('fiber.txt', 0.0), ('choco-ch.ips', 0.0), ('drinks.gui', 0.0), ('fudge.txt', 0.0), ('bread.txt', 0.0), ('oculis.rcp', 0.0), ('pepper.txt', 0.0), ('recepies.fun', 0.0), ('aclamt.txt', 0.0), ('hotel.txt', 0.0), ('boneles2.txt', 0.0), ('dthought.txt', 0.0), ('tpquote2.txt', 0.0), ('tpquotes.txt', 0.0), ('gingbeer.txt', 0.0), ('gameshow.txt', 0.0), ('cooking.jok', 0.0), ('rinaldo.jok', 0.0), ('women.jok', 0.0), ('wrdnws1.txt', 0.0), ('wrdnws2.txt', 0.0), ('wrdnws3.txt', 0.0), ('wrdnws4.txt', 0.0), ('wrdnws5.txt', 0.0), ('wrdnws6.txt', 0.0), ('wrdnws7.txt', 0.0), ('wrdnws8.txt', 0.0), ('wrdnws9.txt', 0.0), ('letterbx.txt', 0.0), ('hierarch.txt', 0.0), ('adt_miam.txt', 0.0), ('gd_flybd.txt', 0.0), ('kanalx.txt', 0.0), ('namm', 0.0), ('earp', 0.0), ('epikarat.txt', 0.0), ('ads.txt', 0.0), ('widows', 0.0), ('twinpeak.txt', 0.0), ('hoonsrc.txt', 0.0), ('lazarus.txt', 0.0), ('barney.cn1', 0.0), ('barney.txt', 0.0), ('bb', 0.0), ('bimg.prn', 0.0), ('bmdn01.txt', 0.0), ('libraway.txt', 0.0), ('wisconsi.txt', 0.0), ('twinkies.jok', 0.0), ('sanshop.txt', 0.0), ('scam.txt', 0.0), ('seeds42.txt', 0.0), ('top10st1.txt', 0.0), ('top10st2.txt', 0.0), ('transp.txt', 0.0), ('mundane.v2', 0.0), ('japantv.txt', 0.0), ('johann', 0.0), ('just2', 0.0), ('bnb_quot.txt', 0.0), ('bored.txt', 0.0), ('sw_err.txt', 0.0), ('skippy.txt', 0.0), ('recip1.txt', 0.0), ('shooters.txt', 0.0), ('slogans.txt', 0.0), ('smurf-03.txt', 0.0), ('smurf_co.txt', 0.0), ('soccer.txt', 0.0), ('beerjesus.hum', 0.0), ('bozo_tv.leg', 0.0), ('diet.txt', 0.0), ('jason.fun', 0.0), ('econridl.fun', 0.0), ('oliver.txt', 0.0), ('oliver02.txt', 0.0), ('sf-zine.pub', 0.0), ('subrdead.hum', 0.0), ('topten.hum', 0.0), ('how2bgod.txt', 0.0), ('hangover.txt', 0.0), ('netmask.txt', 0.0), ('chickens.txt', 0.0), ('classicm.hum', 0.0), ('cmu.share', 0.0), ('commword.hum', 0.0), ('cookbkly.how', 0.0), ('fearcola.hum', 0.0), ('hedgehog.txt', 0.0), ('horflick.txt', 0.0), ('alcohol.hum', 0.0), ('bbh_intv.txt', 0.0), ('beer-g', 0.0), ('beergame.hum', 0.0), ('dining.out', 0.0), ('iced.tea', 0.0), ('thievco.txt', 0.0), ('aeonint.txt', 0.0), ('mindvox', 0.0), ('deathhem.txt', 0.0), ('brownie.rec', 0.0), ('lion.txt', 0.0), ('lp-assoc.txt', 0.0), ('desk.txt', 0.0), ('disclym.txt', 0.0), ('flowchrt', 0.0), ('flux_fix.txt', 0.0), ('packard.txt', 0.0), ('parsnip.txt', 0.0), ('penisprt.txt', 0.0), ('msfields.txt', 0.0), ('mtv.asc', 0.0), ('modstup', 0.0), ('moslem.txt', 0.0), ('mov_rail.txt', 0.0), ('miamimth.txt', 0.0), ('missdish', 0.0), ('consp.txt', 0.0), ('food', 0.0), ('foodtips', 0.0), ('cake.rec', 0.0), ('docdict.txt', 0.0), ('drive.txt', 0.0), ('bhang.fun', 0.0), ('booze2.fun', 0.0), ('cgs_lst.txt', 0.0), ('golnar.txt', 0.0), ('red-neck.jks', 0.0), ('reddwarf.sng', 0.0), ('renorthr.txt', 0.0), ('rns_bcl.txt', 0.0), ('rns_bwl.txt', 0.0), ('rns_ency.txt', 0.0), ('robot.tes', 0.0), ('subb_lis.txt', 0.0), ('staff.txt', 0.0), ('stuf10.txt', 0.0), ('stuf11.txt', 0.0), ('fusion.gal', 0.0), ('sfmovie.txt', 0.0), ('the_ant.txt', 0.0), ('mog-history', 0.0), ('dalive', 0.0), ('iremember', 0.0), ('kilroy', 0.0), ('cform2.txt', 0.0), ('msorrow', 0.0), ('nameisreo.txt', 0.0), ('inquirer.txt', 0.0), ('ins1', 0.0), ('episimp2.txt', 0.0), ('bnbguide.txt', 0.0), ('buldrwho.txt', 0.0), ('gd_alf.txt', 0.0), ('epi_bnb.txt', 0.0), ('epi_tton.txt', 0.0), ('epi_rns.txt', 0.0), ('epi_.txt', 0.0), ('epi_merm.txt', 0.0), ('gd_gal.txt', 0.0), ('twilight.txt', 0.0), ('lost.txt', 0.0), ('outlimit.txt', 0.0), ('scratchy.txt', 0.0), ('gd_drwho.txt', 0.0), ('gd_frasr.txt', 0.0), ('gd_liqtv.txt', 0.0), ('gd_maxhd.txt', 0.0), ('gd_hhead.txt', 0.0), ('gd_ol.txt', 0.0), ('epiquest.txt', 0.0), ('gd_tznew.txt', 0.0), ('ghostsch.hum', 0.0), ('gd_sgrnd.txt', 0.0), ('gd_ql.txt', 0.0), ('a-team', 0.0), ('a_fish_c.apo', 0.0), ('a_tv_t-p.com', 0.0), ('aboutada.txt', 0.0), ('adrian_e.faq', 0.0), ('allfam.epi', 0.0), ('allusion', 0.0), ('amazing.epi', 0.0), ('anime.cli', 0.0), ('anime.lif', 0.0), ('hbo_spec.rev', 0.0), ('highland.epi', 0.0), ('hitchcok.txt', 0.0), ('wacky.ani', 0.0), ('wkrp.epi', 0.0), ('cast.lis', 0.0), ('christop.int', 0.0), ('chung.iv', 0.0), ('clancy.txt', 0.0), ('comic_st.gui', 0.0), ('cultmov.faq', 0.0), ('maxheadr', 0.0), ('facedeth.txt', 0.0), ('farsi.txt', 0.0), ('nukewar.txt', 0.0), ('number', 0.0), ('spoonlis.txt', 0.0), ('sinksub.txt', 0.0), ('sorority.gir', 0.0), ('petshop', 0.0), ('prac2.jok', 0.0), ('pracjoke.txt', 0.0), ('fed.txt', 0.0), ('cybrtrsh.txt', 0.0), ('hstlrtxt.txt', 0.0), ('homermmm.txt', 0.0), ('how2dotv.txt', 0.0), ('filmgoof.txt', 0.0), ('films_gl.txt', 0.0), ('fish.rec', 0.0), ('hitchcoc.app', 0.0), ('is_story.txt', 0.0), ('fegg!int.txt', 0.0), ('feggaqui.txt', 0.0), ('feggmagi.txt', 0.0), ('normquot.txt', 0.0), ('history2.oop', 0.0), ('pol-corr.txt', 0.0), ('poopie.txt', 0.0), ('ppbeer.txt', 0.0), ('pot.txt', 0.0), ('headlnrs', 0.0), ('hell.txt', 0.0), ('hum2', 0.0), ('humor9.txt', 0.0), ('cokeform.txt', 0.0), ('contract.moo', 0.0), ('cookberk', 0.0), ('cookie.1', 0.0), ('cooplaws', 0.0), ('cuisine.txt', 0.0), ('modemwld.txt', 0.0), ('minn.txt', 0.0), ('oam-001.txt', 0.0), ('oam.nfo', 0.0), ('acronym.txt', 0.0), ('college.txt', 0.0), ('english.txt', 0.0), ('figure_1.txt', 0.0), ('crzycred.lst', 0.0), ('arnold.txt', 0.0), ('ateam.epi', 0.0), ('avengers.lis', 0.0), ('bbc_vide.cat', 0.0), ('beauty.tm', 0.0), ('blackadd', 0.0), ('blake7.lis', 0.0), ('doc-says.txt', 0.0), ('dromes.txt', 0.0), ('eatme.txt', 0.0), ('free-cof.fee', 0.0), ('initials.rid', 0.0), ('old.txt', 0.0), ('post.nuc', 0.0), ('quantum.phy', 0.0), ('resolutn.txt', 0.0), ('soporifi.abs', 0.0), ('swearfrn.hum', 0.0), ('tarot.txt', 0.0), ('c0dez.txt', 0.0), ('beer.txt', 0.0), ('butcher.txt', 0.0), ('curiousgeorgie.txt', 0.0), ('freudonseuss.txt', 0.0), ('normalboy.txt', 0.0), ('annoy.fascist', 0.0), ('booze.fun', 0.0), ('diesmurf.txt', 0.0), ('elevator.fun', 0.0), ('get.drunk.cheap', 0.0), ('how.bugs.breakd', 0.0), ('normal.boy', 0.0), ('practica.txt', 0.0), ('dead2.txt', 0.0), ('dead3.txt', 0.0), ('dead4.txt', 0.0), ('dead5.txt', 0.0), ('radexposed.txt', 0.0), ('carowner.txt', 0.0), ('pat.txt', 0.0), ('bugs.txt', 0.0), ('hack7.txt', 0.0), ('happyhack.txt', 0.0), ('airlines', 0.0), ('skincat', 0.0), ('venganza.txt', 0.0), ('venison.txt', 0.0), ('homebrew.txt', 0.0), ('hop.faq', 0.0), ('hotpeper.txt', 0.0), ('necropls.txt', 0.0), ('three.txt', 0.0), ('eggroll1.mea', 0.0), ('dandwine.bev', 0.0), ('engmuffn.txt', 0.0), ('egglentl.vgn', 0.0), ('egg-bred.txt', 0.0), ('focaccia.brd', 0.0), ('feista01.dip', 0.0), ('frogeye1.sal', 0.0), ('firecamp.txt', 0.0), ('goldwatr.txt', 0.0), ('garlpast.vgn', 0.0), ('greenchi.txt', 0.0), ('texbeef.txt', 0.0), ('unochili.txt', 0.0), ('wonton.txt', 0.0), ('woods.txt', 0.0), ('whitbred.txt', 0.0), ('gack!.txt', 0.0), ('yogurt.asc', 0.0), ('insult.lst', 0.0), ('insults1.txt', 0.0), ('calamus.hrb', 0.0), ('damiana.hrb', 0.0), ('imbecile.txt', 0.0), ('orgfrost.bev', 0.0), ('penndtch', 0.0), ('pasta001.sal', 0.0), ('oranchic.pol', 0.0), ('pbcookie.des', 0.0), ('jon.txt', 0.0), ('hackmorality.txt', 0.0), ('whatbbs', 0.0), ('hermsys.txt', 0.0), ('antimead.bev', 0.0), ('appbred.brd', 0.0), ('arcadian.txt', 0.0), ('apsaucke.des', 0.0), ('awespinh.sal', 0.0), ('applepie.des', 0.0), ('banana05.brd', 0.0), ('banana01.brd', 0.0), ('berryeto.bev', 0.0), ('jawgumbo.fis', 0.0), ('jalapast.dip', 0.0), ('batrbred.txt', 0.0), ('beershrp.fis', 0.0), ('banana04.brd', 0.0), ('jawsalad.fis', 0.0), ('banana03.brd', 0.0), ('beershrm.fis', 0.0), ('jambalay.pol', 0.0), ('banana02.brd', 0.0), ('baklava.des', 0.0), ('japice.bev', 0.0), ('jungjuic.bev', 0.0), ('montoys.txt', 0.0), ('seafood.txt', 0.0), ('zuccmush.sal', 0.0), ('strattma.txt', 0.0), ('margos.txt', 0.0), ('shuimai.txt', 0.0), ('stagline.txt', 0.0), ('mitch.txt', 0.0), ('zucantom.sal', 0.0), ('brdpudd.des', 0.0), ('breadpud.des', 0.0), ('btscke03.des', 0.0), ('btscke02.des', 0.0), ('btscke01.des', 0.0), ('blkbean.txt', 0.0), ('boarchil.txt', 0.0), ('bredcake.des', 0.0), ('bread.rcp', 0.0), ('blkbnsrc.vgn', 0.0), ('bunacald.fis', 0.0), ('burrito.mea', 0.0), ('caramels.des', 0.0), ('caesardr.sal', 0.0), ('btscke05.des', 0.0), ('buffwing.pol', 0.0), ('capital.txt', 0.0), ('butstcod.fis', 0.0), ('btscke04.des', 0.0), ('advrtize.txt', 0.0), ('bitnet.txt', 0.0), ('nzdrinks.txt', 0.0), ('recipe.008', 0.0), ('renored.txt', 0.0), ('recipe.009', 0.0), ('recipe.005', 0.0), ('recipe.006', 0.0), ('recipe.003', 0.0), ('recipe.004', 0.0), ('recipe.012', 0.0), ('recipe.002', 0.0), ('recipe.001', 0.0), ('recipe.007', 0.0), ('oakwood.txt', 0.0), ('recipe.010', 0.0), ('richbred.txt', 0.0), ('recipe.011', 0.0), ('oldtime.txt', 0.0), ('all_grai', 0.0), ('amchap2.txt', 0.0), ('beer.gam', 0.0), ('beergame.txt', 0.0), ('beer-gui', 0.0), ('gotukola.hrb', 0.0), ('hitler.txt', 0.0), ('coollngo2.txt', 0.0), ('jimhood.txt', 0.0), ('aphrodis.txt', 0.0), ('antibiot.txt', 0.0), ('curry.txt', 0.0), ('bw.txt', 0.0), ('atherosc.txt', 0.0), ('booknuti.txt', 0.0), ('chinese.txt', 0.0), ('ayurved.txt', 0.0), ('brush1.txt', 0.0), ('anorexia.txt', 0.0), ('back1.txt', 0.0), ('critic.txt', 0.0), ('cereal.txt', 0.0), ('arthriti.txt', 0.0), ('beesherb.txt', 0.0), ('aniherb.txt', 0.0), ('acetab1.txt', 0.0), ('blood.txt', 0.0), ('1st_aid.txt', 0.0), ('acne1.txt', 0.0), ('proudlyserve.txt', 0.0), ('jayjay.txt', 0.0), ('bnbeg2.4.txt', 0.0), ('att.txt', 0.0), ('calculus.txt', 0.0), ('byfb.txt', 0.0), ('gumbo.txt', 0.0), ('btaco.txt', 0.0), ('beerwarn.txt', 0.0), ('qttofu.vgn', 0.0), ('paddingurpapers.txt', 0.0), ('religion.txt', 0.0), ('girlspeak.txt', 0.0), ('enlightenment.txt', 0.0), ('cartoon_laws.txt', 0.0), ('llamas.txt', 0.0), ('ganamembers.txt', 0.0), ('coffeebeerwomen.txt', 0.0), ('confucius_say.txt', 0.0), ('aggie.txt', 0.0), ('collected_quotes.txt', 0.0), ('trekfume.txt', 0.0), ('labels.txt', 0.0), ('horoscope.txt', 0.0), ('fireplacein.txt', 0.0), ('childrenbooks.txt', 0.0), ('turing.shr', 0.0), ('valujet.txt', 0.0), ('computer.txt', 0.0), ('jokes1.txt', 0.0), ('lawyers.txt', 0.0), ('cops.txt', 0.0), ('fbipizza.txt', 0.0), ('mcd.txt', 0.0), ('nasaglenn.txt', 0.0), ('bhb.ill', 0.0), ('bond-2.txt', 0.0), ('indgrdn.txt', 0.0), ('quantity.001', 0.0), ('sawyer.txt', 0.0), ('exidy.txt', 0.0), ('deadlysins.txt', 0.0), ('yuban.txt', 0.0), ('planetzero.txt', 0.0), ('aids.txt', 0.0), ('reeves.txt', 0.0), ('moore.txt', 0.0), ('chickenheadbbs.txt', 0.0), ('phxbbs-m.txt', 0.0), ('ukunderg.txt', 0.0), ('silverclaws.txt', 0.0), ('namaste.txt', 0.0), ('crazy.txt', 0.0), ('lifeonledge.txt', 0.0), ('lipkovits.txt', 0.0), ('basehead.txt', 0.0), ('draxamus.txt', 0.0), ('stressman.txt', 0.0), ('exylic.txt', 0.0), ('apsnet.txt', 0.0), ('lansing.txt', 0.0), ('heroic.txt', 0.0), ('onan.txt', 0.0), ('teevee.hum', 0.0), ('disclmr.txt', 0.0), ('grail.txt', 0.0), ('mel.txt', 0.0), ('hacktest.txt', 0.0), ('hackingcracking.txt', 0.0), ('analogy.hum', 0.0), ('saveface.hum', 0.0), ('adameve.hum', 0.0), ('mlverb.hum', 0.0), ('drugshum.hum', 0.0), ('modest.hum', 0.0), ('ghostfun.hum', 0.0), ('cheapfar.hum', 0.0), ('parades.hum', 0.0), ('bingbong.hum', 0.0), ('charity.hum', 0.0), ('poets.hum', 0.0), ('reconcil.hum', 0.0), ('solders.hum', 0.0), ('brainect.hum', 0.0), ('weights.hum', 0.0), ('shameonu.hum', 0.0), ('memory.hum', 0.0), ('watchlip.hum', 0.0), ('symbol.hum', 0.0), ('kaboom.hum', 0.0), ('spacever.hum', 0.0), ('whatthe.hum', 0.0), ('hammock.hum', 0.0), ('throwawa.hum', 0.0), ('sigs.txt', 0.0), ('nigel.1', 0.0), ('adcopy.hum', 0.0), ('b-2.jok', 0.0), ('zen.txt', 0.0), ('acronym.lis', 0.0), ('admin.txt', 0.0), ('abbott.txt', 0.0), ('suicide2.txt', 0.0), ('alcatax.txt', 0.0), ('manager.txt', 0.0), ('alabama.txt', 0.0), ('boatmemo.jok', 0.0), ('alflog.txt', 0.0), ('cancer.rat', 0.0), ('atombomb.hum', 0.0), ('badday.hum', 0.0), ('answers', 0.0), ('nigel10.txt', 0.0), ('bank.rob', 0.0), ('beer.hum', 0.0), ('argotdic.txt', 0.0), ('bitchcar.hum', 0.0), ('catin.hat', 0.0), ('blackapp.hum', 0.0), ('beapimp.hum', 0.0), ('bigpic1.hum', 0.0), ('beave.hum', 0.0), ('boe.hum', 0.0), ('cabbage.txt', 0.0), ('buzzword.hum', 0.0), ('cold.fus', 0.0), ('butwrong.hum', 0.0), ('blaster.hum', 0.0), ('chickens.jok', 0.0), ('blackhol.hum', 0.0), ('cheapin.la', 0.0), ('bugbreak.hum', 0.0), ('btcisfre.hum', 0.0), ('browneco.hum', 0.0), ('cbmatic.hum', 0.0), ('macsfarm.old', 0.0), ('catranch.hum', 0.0), ('catballs.hum', 0.0), ('calif.hum', 0.0), ('change.hum', 0.0), ('nihgel_8.9', 0.0), ('cockney.alp', 0.0), ('merry.txt', 0.0), ('chinesec.hum', 0.0), ('malechem.txt', 0.0), ('college.sla', 0.0), ('manners.txt', 0.0), ('arab.dic', 0.0), ('number_k.ill', 0.0), ('coldfake.hum', 0.0), ('comrevi1.hum', 0.0), ('making_y.wel', 0.0), ('cucumber.jok', 0.0), ('madhattr.jok', 0.0), ('nukewar.jok', 0.0), ('anthropo.stu', 0.0), ('d-ned.hum', 0.0), ('cuchy.hum', 0.0), ('failure.txt', 0.0), ('cowexplo.hum', 0.0), ('addrmeri.txt', 0.0), ('dark.suc', 0.0), ('rabbit.txt', 0.0), ('readme.bat', 0.0), ('deterior.hum', 0.0), ('disaster.hum', 0.0), ('engineer.hum', 0.0), ('enquire.hum', 0.0), ('roadpizz.txt', 0.0), ('age.txt', 0.0), ('dingding.hum', 0.0), ('art-fart.hum', 0.0), ('acronyms.txt', 0.0), ('bad-d', 0.0), ('druggame.hum', 0.0), ('defectiv.hum', 0.0), ('murphy_l.txt', 0.0), ('doggun.sto', 0.0), ('record_.gap', 0.0), ('beerdiag.txt', 0.0), ('firstaid.txt', 0.0), ('bad.jok', 0.0), ('drinking.tro', 0.0), ('finalexm.hum', 0.0), ('relative.ada', 0.0), ('moose.txt', 0.0), ('revolt.dj', 0.0), ('residncy.jok', 0.0), ('middle.age', 0.0), ('forsooth.hum', 0.0), ('footfun.hum', 0.0), ('from.hum', 0.0), ('freshman.hum', 0.0), ('resrch_p.hra', 0.0), ('flattax.hum', 0.0), ('fwksfun.hum', 0.0), ('female.jok', 0.0), ('goforth.hum', 0.0), ('men&wome.txt', 0.0), ('mensroom.jok', 0.0), ('bbq.txt', 0.0), ('mowers.txt', 0.0)]\n"
          ]
        }
      ],
      "source": [
        "sorted_x = sorted(dict_jaccard.items(), key=operator.itemgetter(1),reverse=True)\n",
        "print(sorted_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8a65UwOY_aa"
      },
      "source": [
        "**printing top 5 matching documents values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVUmpqcyBAUm",
        "outputId": "30a2496b-54f7-4c84-f087-4350f61caa36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 bread.rec 0.010752688172043012\n",
            "2 insect1.txt 0.0053475935828877\n",
            "3 booze1.fun 0.003703703703703704\n",
            "4 drinks.txt 0.0034602076124567475\n",
            "5 coffee.txt 0.0024330900243309003\n"
          ]
        }
      ],
      "source": [
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SekO72vmhcIy"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA9BHFyukpn_"
      },
      "outputs": [],
      "source": [
        "def process2(txt):\n",
        "\n",
        "  #punctuation removal\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~=+'''\n",
        "  for ele in txt:\n",
        "    if ele in punc:\n",
        "        txt = txt.replace(ele, \" \")\n",
        "  \n",
        "  #lower the text\n",
        "  txt_lower=txt.lower()\n",
        "\n",
        "  #remove stopwords\n",
        "  txt_stopwords=remove_stopwords(txt_lower)\n",
        "  \n",
        "  #tokenization\n",
        "  txt_tokenize=nltk.word_tokenize(txt_stopwords)\n",
        "\n",
        "  #remove blank spaces\n",
        "  for word in txt_tokenize:\n",
        "    word=word.strip()  \n",
        "\n",
        "  #remove duplicates \n",
        "  #txt_tokenize =set(txt_tokenize)\n",
        "  \n",
        "  return txt_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgZwbBOwlB7I"
      },
      "outputs": [],
      "source": [
        "frequency_dict={}\n",
        "unique_words=set()\n",
        "for idx in range(0,len(my_list1)) :\n",
        "  file=open(\"/content/gdrive/MyDrive/Humor,Hist,Media,Food/\"+my_list1[idx],\"r\",errors=\"ignore\",encoding =\"utf-8\")\n",
        "  my_txt=file.read()\n",
        "  token_list = process2(my_txt)\n",
        "  unique_words.update(token_list)\n",
        "  for token in token_list:\n",
        "    if token in frequency_dict:\n",
        "      if my_list1[idx] in frequency_dict[token]:\n",
        "        frequency_dict[token][my_list1[idx]]=frequency_dict[token][my_list1[idx]]+1\n",
        "      else:\n",
        "        frequency_dict[token][my_list1[idx]]=1\n",
        "    else:\n",
        "      frequency_dict[token]={}\n",
        "      frequency_dict[token][my_list1[idx]]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZOI9GaPnPvT"
      },
      "outputs": [],
      "source": [
        "word_list=[]\n",
        "for i in unique_words:\n",
        "  word_list.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF37_NGWwg1X"
      },
      "outputs": [],
      "source": [
        "#code to create two dictionary having document-list and document-sum mapping\n",
        "docu_list={}\n",
        "for i in my_list1:\n",
        "  for j in word_list:\n",
        "    if i in docu_list:\n",
        "      if i in frequency_dict[j]:\n",
        "        docu_list[i].append(frequency_dict[j][i])\n",
        "    else:\n",
        "      docu_list[i]=[]\n",
        "      if i in frequency_dict[j]:\n",
        "        docu_list[i].append(frequency_dict[j][i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flO2qsCq0HQ2"
      },
      "outputs": [],
      "source": [
        "docu_sum={}\n",
        "for i in my_list1:\n",
        "  for j in word_list:\n",
        "    if i in docu_sum:\n",
        "      if i in frequency_dict[j]:\n",
        "        docu_sum[i]=docu_sum[i]+frequency_dict[j][i]\n",
        "    else:\n",
        "      if i in frequency_dict[j]:\n",
        "        docu_sum[i]=frequency_dict[j][i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foKfRYbQ1PV1",
        "outputId": "e54464ac-8724-42da-ec6e-38e66b04dc5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hell.jok': 283,\n",
              " 'hate.hum': 407,\n",
              " 'rinaldos.txt': 290,\n",
              " 'gohome.hum': 400,\n",
              " 'harmful.hum': 319,\n",
              " 'herb!.hum': 513,\n",
              " 'mothers.txt': 183,\n",
              " 'grommet.hum': 273,\n",
              " 'conan.txt': 1146,\n",
              " 'coyote.txt': 1105,\n",
              " 'murph.jok': 669,\n",
              " 'hi.tec': 827,\n",
              " 'icm.hum': 167,\n",
              " 'imprrisk.hum': 893,\n",
              " 'roach.asc': 88,\n",
              " 'howlong.hum': 614,\n",
              " 'interv.hum': 410,\n",
              " 'investi.hum': 405,\n",
              " 'bless.bc': 150,\n",
              " 'incarhel.hum': 192,\n",
              " 'insure.hum': 239,\n",
              " 'insanity.hum': 628,\n",
              " 'dirtword.txt': 502,\n",
              " 'horoscop.jok': 337,\n",
              " 'impurmat.hum': 340,\n",
              " 'cartoon_.txt': 306,\n",
              " 'bible.txt': 53,\n",
              " 'murphy.txt': 405,\n",
              " 'japrap.hum': 191,\n",
              " 'test.jok': 825,\n",
              " 'dym': 1533,\n",
              " 'jac&tuu.hum': 910,\n",
              " 'ivan.hum': 480,\n",
              " 'killer.hum': 124,\n",
              " 'terms.hum': 599,\n",
              " 'testchri.txt': 341,\n",
              " 'kid_diet.txt': 365,\n",
              " 'test2.jok': 408,\n",
              " 'killself.hum': 789,\n",
              " 'kilsmur.hum': 306,\n",
              " 'motrbike.jok': 236,\n",
              " 'lbinter.hum': 2547,\n",
              " 'lawsuniv.hum': 296,\n",
              " 'blooprs1.asc': 972,\n",
              " 'ludeinfo.txt': 828,\n",
              " 'simp.txt': 242,\n",
              " 'legal.hum': 495,\n",
              " 'murphys.txt': 1764,\n",
              " 'ludeinfo.hum': 754,\n",
              " 'losers86.hum': 930,\n",
              " 'lifeinfo.hum': 520,\n",
              " 'lll.hum': 480,\n",
              " 'luggage.hum': 461,\n",
              " 'looser.hum': 1503,\n",
              " 'livnware.hum': 269,\n",
              " 'lozerzon.hum': 1037,\n",
              " 'lif&love.hum': 562,\n",
              " 'phunatdi.ana': 549,\n",
              " 'lozeuser.hum': 3776,\n",
              " 'office.txt': 164,\n",
              " 'lobquad.hum': 108,\n",
              " 'catstory.txt': 173,\n",
              " 'losers84.hum': 2129,\n",
              " 'lifeimag.hum': 819,\n",
              " 'llong.hum': 2001,\n",
              " 'makebeer.hum': 392,\n",
              " 'manilla.hum': 154,\n",
              " 'meinkamp.hum': 763,\n",
              " 'm0dzmen.hum': 1219,\n",
              " 'pun.txt': 766,\n",
              " 'mtm.hum': 1086,\n",
              " 'maecenas.hum': 928,\n",
              " 'melodram.hum': 1363,\n",
              " 'manspace.hum': 5701,\n",
              " 'luzerzo2.hum': 1052,\n",
              " 'mash.hum': 3019,\n",
              " 'marines.hum': 145,\n",
              " 'madscrib.hum': 1182,\n",
              " 'mailfrag.hum': 338,\n",
              " 'memo.hum': 105,\n",
              " 'montpyth.hum': 2741,\n",
              " 'reasons.txt': 272,\n",
              " 'misery.hum': 193,\n",
              " 'calvin.txt': 554,\n",
              " 'mutate.hum': 267,\n",
              " 'mydaywss.hum': 1073,\n",
              " 'miamadvi.hum': 284,\n",
              " 'mrscienc.hum': 824,\n",
              " 'miami.hum': 687,\n",
              " 'miranda.hum': 276,\n",
              " 'f_tang.txt': 624,\n",
              " 'missheav.hum': 330,\n",
              " 'newconst.hum': 319,\n",
              " 'nuke.hum': 2060,\n",
              " 'naivewiz.hum': 177,\n",
              " 'nysucks.hum': 197,\n",
              " 'novel.hum': 1427,\n",
              " 'boston.geog': 26,\n",
              " 'cartoon.law': 329,\n",
              " 'cars.txt': 342,\n",
              " 'myheart.hum': 1250,\n",
              " 'cartwb.son': 391,\n",
              " 'nukeplay.hum': 339,\n",
              " 'nurds.hum': 264,\n",
              " 'news.hum': 306,\n",
              " 'newmex.hum': 247,\n",
              " 'o-ttalk.hum': 476,\n",
              " 'onetotwo.hum': 3845,\n",
              " 'ozarks.hum': 283,\n",
              " 'oilgluts.hum': 247,\n",
              " 'odearakk.hum': 97,\n",
              " 'oldeng.hum': 385,\n",
              " 'p-law.hum': 146,\n",
              " 'opinion.hum': 210,\n",
              " 'chunnel.txt': 201,\n",
              " 'ookpik.hum': 105,\n",
              " 'deep.txt': 683,\n",
              " 'ohandre.hum': 146,\n",
              " 'onetoone.hum': 4422,\n",
              " 'devils.jok': 1079,\n",
              " 'planeget.hum': 466,\n",
              " 'peatchp.hum': 2541,\n",
              " 'dieter.txt': 274,\n",
              " 'phorse.hum': 551,\n",
              " 'prayer.hum': 114,\n",
              " 'poll2res.hum': 616,\n",
              " 'popconc.hum': 273,\n",
              " 'policpig.hum': 255,\n",
              " 'pizzawho.hum': 1187,\n",
              " 'parabl.hum': 187,\n",
              " 'prawblim.hum': 319,\n",
              " 'passage.hum': 2123,\n",
              " 'popmusi.hum': 335,\n",
              " 'phony.hum': 272,\n",
              " 'donut.txt': 946,\n",
              " 'raven.hum': 327,\n",
              " 'ratspit.hum': 170,\n",
              " 'reagan.hum': 83,\n",
              " 'psilaine.hum': 1163,\n",
              " 'dead-r': 234,\n",
              " 'radiolaf.hum': 438,\n",
              " 'quest.hum': 3882,\n",
              " 'pro-fact.hum': 2629,\n",
              " 'fuckyou2.txt': 941,\n",
              " 'ratings.hum': 209,\n",
              " 'rapmastr.hum': 360,\n",
              " 'rocking.hum': 557,\n",
              " 'report.hum': 119,\n",
              " 'reddye.hum': 292,\n",
              " 'drinkrul.jok': 227,\n",
              " 'docspeak.txt': 227,\n",
              " 'rockmus.hum': 842,\n",
              " 'research.hum': 156,\n",
              " 'rentals.hum': 308,\n",
              " 'repair.hum': 154,\n",
              " 'ripoffpc.hum': 954,\n",
              " 'insult': 136,\n",
              " 'drunk.txt': 528,\n",
              " 'corporat.txt': 103,\n",
              " 't_zone.jok': 731,\n",
              " 'dubltalk.jok': 365,\n",
              " 'takenote.jok': 165,\n",
              " 'social.hum': 488,\n",
              " 'spydust.hum': 402,\n",
              " 'jokes': 2360,\n",
              " 'socecon.hum': 1137,\n",
              " 'solviets.hum': 211,\n",
              " 'smurfkil.hum': 1549,\n",
              " 'smackjok.hum': 1036,\n",
              " 'spider.hum': 155,\n",
              " 'iqtest': 259,\n",
              " 'cucumber.txt': 180,\n",
              " 'skippy.hum': 402,\n",
              " 'terbear.txt': 505,\n",
              " 'soleleer.hum': 1482,\n",
              " 'teens.txt': 209,\n",
              " 'shuttleb.hum': 187,\n",
              " 'stone.hum': 2026,\n",
              " 'taping.hum': 460,\n",
              " 'talebeat.hum': 1172,\n",
              " 'y.txt': 241,\n",
              " 't-10.hum': 201,\n",
              " 'standard.hum': 425,\n",
              " 't-shirt.hum': 441,\n",
              " \"terrmcd'.hum\": 598,\n",
              " 'sungenu.hum': 731,\n",
              " 'termpoem.txt': 98,\n",
              " 'texican.dic': 1589,\n",
              " 'top10.txt': 3332,\n",
              " 'televisi.hum': 477,\n",
              " 'languag.jok': 64,\n",
              " 'timetr.hum': 521,\n",
              " 'hecomes.jok': 292,\n",
              " 'thecube.hum': 431,\n",
              " 'textgrap.hum': 105,\n",
              " 'tfepisod.hum': 524,\n",
              " 'terrnieg.hum': 266,\n",
              " 'the_math.hel': 461,\n",
              " 'tfpoems.hum': 145,\n",
              " 'lawyer.jok': 2152,\n",
              " 'tickmoon.hum': 647,\n",
              " 'toxcwast.hum': 379,\n",
              " 'let.go': 512,\n",
              " 'test.hum': 443,\n",
              " 'koans.txt': 535,\n",
              " 'cartoon.laws': 324,\n",
              " 'tribble.hum': 217,\n",
              " 'whoops.hum': 194,\n",
              " 'turbo.hum': 261,\n",
              " 'truths.hum': 220,\n",
              " 'wetdream.hum': 270,\n",
              " 'whoon1st.hum': 661,\n",
              " 'wedding.hum': 142,\n",
              " 'wagon.hum': 508,\n",
              " 'truthlsd.hum': 1328,\n",
              " 'voltron.hum': 224,\n",
              " 'trekwes.hum': 569,\n",
              " 'why-me.hum': 520,\n",
              " 'twinkie.txt': 205,\n",
              " 'thesis.beh': 565,\n",
              " 'thermite.ana': 976,\n",
              " 'liceprof.sty': 129,\n",
              " 'word.hum': 184,\n",
              " 'yuppies.hum': 498,\n",
              " 'who.txt': 122,\n",
              " 'yjohncse.hum': 331,\n",
              " 'jeffie.heh': 133,\n",
              " 'zodiac.hum': 244,\n",
              " 'worldend.hum': 1101,\n",
              " 'billcat.hum': 356,\n",
              " 'xtermin8.hum': 405,\n",
              " 'hotnnot.hum': 649,\n",
              " 'trukdeth.txt': 331,\n",
              " 'limerick.jok': 721,\n",
              " 'vaguemag.90s': 583,\n",
              " 'wagit.txt': 244,\n",
              " 'washroom.txt': 231,\n",
              " 'tshirts.jok': 629,\n",
              " 'univ.odd': 1157,\n",
              " 'lines.jok': 1038,\n",
              " 'urban.txt': 3866,\n",
              " 'st_silic.txt': 451,\n",
              " 'waitress.txt': 342,\n",
              " 'units.mea': 204,\n",
              " 'math.2': 923,\n",
              " 'wimptest.txt': 618,\n",
              " 'lion.jok': 983,\n",
              " 'weight.txt': 137,\n",
              " 'misc.1': 1944,\n",
              " 'marriage.hum': 257,\n",
              " 'woodbugs.txt': 992,\n",
              " 'welfare.txt': 131,\n",
              " 'strine.txt': 4277,\n",
              " 'math.far': 320,\n",
              " 'yogisays.txt': 191,\n",
              " 'math.1': 830,\n",
              " 'lotsa.jok': 1870,\n",
              " 'zgtoilet.txt': 450,\n",
              " 'climbing.let': 2278,\n",
              " 'church.sto': 357,\n",
              " 'xibovac.txt': 6175,\n",
              " 'chainltr.txt': 136,\n",
              " 'childhoo.jok': 1462,\n",
              " 'lozers': 720,\n",
              " 'co-car.jok': 162,\n",
              " 'commutin.jok': 995,\n",
              " 'woolly_m.amm': 461,\n",
              " 'netnews.10': 138,\n",
              " 'hack': 314,\n",
              " 'element.jok': 132,\n",
              " 'nigel.3': 794,\n",
              " 'nigel.2': 823,\n",
              " 'electric.txt': 162,\n",
              " 'court.quips': 145,\n",
              " 'nigel.10': 1402,\n",
              " 'eandb.drx': 283,\n",
              " 'elephant.fun': 424,\n",
              " 'fartinfo.txt': 712,\n",
              " 'nigel.6': 784,\n",
              " 'drinker.txt': 176,\n",
              " 'fascist.txt': 814,\n",
              " 'final-ex.txt': 424,\n",
              " 'idr2.txt': 275,\n",
              " 'jokes.txt': 1716,\n",
              " 'ourfathr.txt': 109,\n",
              " 'nigel.5': 556,\n",
              " 'exam.50': 654,\n",
              " 'nigel.4': 730,\n",
              " 'fartting.txt': 264,\n",
              " 'excuses.txt': 231,\n",
              " 'lawskool.txt': 403,\n",
              " 'german.aut': 338,\n",
              " 'passenge.sim': 288,\n",
              " 'fusion.sup': 582,\n",
              " 'good.txt': 130,\n",
              " 'gd_guide.txt': 789,\n",
              " 'fuck!.txt': 234,\n",
              " 'grammar.jok': 267,\n",
              " 'grospoem.txt': 575,\n",
              " 'nigel.7': 697,\n",
              " 'rec.por': 275,\n",
              " 'lawhunt.txt': 283,\n",
              " 'gas.txt': 943,\n",
              " 'polemom.txt': 220,\n",
              " 'polly_.new': 267,\n",
              " 'poli_t.ics': 65,\n",
              " 'problem.txt': 240,\n",
              " 'dover.poem': 93,\n",
              " 'potty.txt': 366,\n",
              " 'moonshin': 353,\n",
              " 'pepsideg.txt': 427,\n",
              " 'polly.txt': 340,\n",
              " 'nintendo.jok': 561,\n",
              " 'proof.met': 285,\n",
              " 'pournell.spo': 392,\n",
              " 'pickup.lin': 170,\n",
              " 'pukeprom.jok': 517,\n",
              " 'prooftec.txt': 121,\n",
              " 'pure.mat': 162,\n",
              " 'psalm_re.aga': 99,\n",
              " 'proposal.jok': 208,\n",
              " 'leech.txt': 302,\n",
              " 'python_s.ong': 74,\n",
              " 'psych_pr.quo': 332,\n",
              " 'prover_w.iso': 259,\n",
              " 'curse.txt': 539,\n",
              " 'peanuts.txt': 205,\n",
              " 'college.hum': 270,\n",
              " 'psalm23.txt': 108,\n",
              " 'puzzles.jok': 134,\n",
              " 'quux_p.oem': 4596,\n",
              " 'tuflife.txt': 214,\n",
              " 'eskimo.nel': 783,\n",
              " 'pickup.txt': 1055,\n",
              " 'prac1.jok': 4076,\n",
              " 'flowchrt.txt': 56,\n",
              " 'hitler.59': 537,\n",
              " 'pecker.txt': 208,\n",
              " 'smartass.txt': 490,\n",
              " 'hilbilly.wri': 123,\n",
              " 'height.txt': 80,\n",
              " 'pipespec.txt': 177,\n",
              " 'hitlerap.txt': 191,\n",
              " 'insuranc.sty': 502,\n",
              " 'htswfren.txt': 806,\n",
              " 'prac3.jok': 4039,\n",
              " 'snipe.txt': 177,\n",
              " 'stereo.txt': 351,\n",
              " 'hoosier.txt': 215,\n",
              " 'televisi.txt': 551,\n",
              " 'humatra.txt': 724,\n",
              " 'steroid.txt': 279,\n",
              " 'inlaws1.txt': 916,\n",
              " 'idaho.txt': 1156,\n",
              " 'law.sch': 331,\n",
              " 'humatran.jok': 773,\n",
              " 'smokers.txt': 140,\n",
              " 'horoscop.txt': 258,\n",
              " 'how_to_i.pro': 226,\n",
              " 'sysadmin.txt': 1418,\n",
              " 'woodbine.txt': 270,\n",
              " 'letter.txt': 2326,\n",
              " 'letter_f.sch': 175,\n",
              " 'laws.txt': 788,\n",
              " 'kloo.txt': 872,\n",
              " 'psycho.txt': 655,\n",
              " 'letgosh.txt': 434,\n",
              " 'sysman.txt': 492,\n",
              " 'jargon.phd': 237,\n",
              " 'lampoon.jok': 159,\n",
              " 'jokeju07.txt': 134,\n",
              " 'italoink.txt': 166,\n",
              " 'jc-elvis.inf': 431,\n",
              " 'rednecks.txt': 243,\n",
              " 'realest.txt': 357,\n",
              " 'primes.jok': 222,\n",
              " 'odd_to.obs': 262,\n",
              " 'lions.cat': 1266,\n",
              " 'oxymoron.jok': 776,\n",
              " 'signatur.jok': 959,\n",
              " 'luvstory.txt': 5002,\n",
              " 'oracle.jok': 391,\n",
              " 'smurfs.cc': 295,\n",
              " 'socks.drx': 275,\n",
              " 'prac4.jok': 2214,\n",
              " 'nukwaste': 408,\n",
              " 'smiley.txt': 691,\n",
              " 'one.par': 266,\n",
              " 'farsi.phrase': 172,\n",
              " 'lucky.cha': 287,\n",
              " 'some_hu.mor': 633,\n",
              " 'spelin_r.ifo': 148,\n",
              " 'puzzle.spo': 430,\n",
              " 'jrrt.riddle': 96,\n",
              " 'studentb.txt': 320,\n",
              " 'startrek.txt': 209,\n",
              " 'strsdiet.txt': 138,\n",
              " 'humpty.dumpty': 110,\n",
              " 'telecom.q': 1392,\n",
              " 'princess.brd': 682,\n",
              " 'mr.rogers': 378,\n",
              " 'rent-a_cat': 392,\n",
              " 'psalm_nixon': 99,\n",
              " 'nosuch_nasfic': 730,\n",
              " 'quick.jok': 740,\n",
              " 'un.happy': 344,\n",
              " 'prover.wisom': 258,\n",
              " 'poli.tics': 64,\n",
              " 'shrink.news': 160,\n",
              " 'psalm.reagan': 99,\n",
              " 'number.killer': 667,\n",
              " 'popmach': 72,\n",
              " 'resrch_phrase': 95,\n",
              " 'quantum.jok': 385,\n",
              " 'rinaldos.law': 406,\n",
              " 'quotes.txt': 290,\n",
              " 'top10.elf': 66,\n",
              " 'welfare': 145,\n",
              " 'progrs.gph': 1098,\n",
              " 'squids.gph': 557,\n",
              " 'tnd.1': 2385,\n",
              " 'temphell.jok': 116,\n",
              " 'racist.net': 922,\n",
              " 'quotes.jok': 1464,\n",
              " 'texican.lex': 1589,\n",
              " 'quotes.bug': 220,\n",
              " 'shorties.jok': 1217,\n",
              " 'speling.msk': 283,\n",
              " 'aussie.lng': 750,\n",
              " 'bw-summe.hat': 133,\n",
              " 'bw-phwan.hat': 213,\n",
              " 'justify': 51,\n",
              " 'wisdom': 474,\n",
              " 'outawork.erl': 656,\n",
              " 'la_times.hun': 380,\n",
              " 'firstaid.inf': 347,\n",
              " 'supermar.rul': 239,\n",
              " 'ambrose.bie': 468,\n",
              " 'oasis': 724,\n",
              " 'oldtime.sng': 572,\n",
              " 'gaiahuma': 100,\n",
              " 'oxymoron.txt': 79,\n",
              " 'kid2': 56,\n",
              " 'anim_lif.txt': 1922,\n",
              " 'bad': 1549,\n",
              " 'wood': 409,\n",
              " 'talkbizr.txt': 1643,\n",
              " 'woodsmok.txt': 505,\n",
              " 'bagelope.txt': 157,\n",
              " 'various.txt': 3055,\n",
              " 'variety3.asc': 3012,\n",
              " 'engrhyme.txt': 147,\n",
              " 'english': 685,\n",
              " 'variety2.asc': 1274,\n",
              " 'empeval.txt': 169,\n",
              " 'epitaph': 439,\n",
              " 'variety1.asc': 1348,\n",
              " 'excuse.txt': 513,\n",
              " 'excuse30.txt': 627,\n",
              " 'vegkill.txt': 159,\n",
              " 'quack26.txt': 7691,\n",
              " 'godmonth.txt': 188,\n",
              " 'vonthomp': 89,\n",
              " 'q.pun': 522,\n",
              " 'cogdis.txt': 696,\n",
              " 'gown.txt': 686,\n",
              " 'growth.txt': 151,\n",
              " 'brewing': 1953,\n",
              " 'hamburge.nam': 466,\n",
              " 'candybar.fun': 594,\n",
              " 'snapple.rum': 597,\n",
              " 'tuna.lab': 260,\n",
              " 'b12.txt': 2255,\n",
              " 'cooking.fun': 353,\n",
              " 'mrsfield': 187,\n",
              " 'coladrik.txt': 436,\n",
              " 'jerky.rcp': 2355,\n",
              " 'coladrik.fun': 362,\n",
              " 'bakebred.txt': 840,\n",
              " 'chili.txt': 5097,\n",
              " 'beginn.ers': 932,\n",
              " 'coke.txt': 145,\n",
              " 'newcoke.txt': 879,\n",
              " 'meat2.txt': 979,\n",
              " 'mead.rcp': 3972,\n",
              " 'vegan.rcp': 11073,\n",
              " 'coke1': 387,\n",
              " 'oatbran.rec': 80,\n",
              " 'appetiz.rcp': 6623,\n",
              " 'coke.fun': 255,\n",
              " 'curry.hrb': 454,\n",
              " 'x-drinks.txt': 4550,\n",
              " 'kashrut.txt': 2755,\n",
              " 'fajitas.rcp': 2092,\n",
              " 'turkey.fun': 541,\n",
              " 'coke_fan.naz': 279,\n",
              " 'fiber.txt': 2610,\n",
              " 'choco-ch.ips': 196,\n",
              " 'drinks.gui': 4615,\n",
              " 'fudge.txt': 171,\n",
              " 'bread.txt': 369,\n",
              " 'oculis.rcp': 2916,\n",
              " 'pepper.txt': 564,\n",
              " 'recepies.fun': 214,\n",
              " 'candy.txt': 49897,\n",
              " 'aclamt.txt': 287,\n",
              " 'hotel.txt': 426,\n",
              " 'boneles2.txt': 1273,\n",
              " 'dthought.txt': 683,\n",
              " 'tpquote2.txt': 276,\n",
              " 'tpquotes.txt': 3230,\n",
              " 'gingbeer.txt': 338,\n",
              " 'gameshow.txt': 2332,\n",
              " 'cooking.jok': 127,\n",
              " 'rinaldo.jok': 301,\n",
              " 'women.jok': 218,\n",
              " 'wrdnws1.txt': 451,\n",
              " 'wrdnws2.txt': 417,\n",
              " 'wrdnws3.txt': 467,\n",
              " 'wrdnws4.txt': 471,\n",
              " 'wrdnws5.txt': 451,\n",
              " 'wrdnws6.txt': 456,\n",
              " 'wrdnws7.txt': 449,\n",
              " 'wrdnws8.txt': 414,\n",
              " 'wrdnws9.txt': 459,\n",
              " 'letterbx.txt': 473,\n",
              " 'hierarch.txt': 132,\n",
              " 'adt_miam.txt': 200,\n",
              " 'gd_flybd.txt': 1905,\n",
              " 'kanalx.txt': 1576,\n",
              " 'namm': 183,\n",
              " 'earp': 574,\n",
              " 'epikarat.txt': 5743,\n",
              " 'ads.txt': 785,\n",
              " 'widows': 350,\n",
              " 'twinpeak.txt': 1289,\n",
              " 'hoonsrc.txt': 254,\n",
              " 'lazarus.txt': 257,\n",
              " 'barney.cn1': 309,\n",
              " 'barney.txt': 5638,\n",
              " 'bb': 402,\n",
              " 'bimg.prn': 1186,\n",
              " 'bmdn01.txt': 1064,\n",
              " 'libraway.txt': 283,\n",
              " 'wisconsi.txt': 555,\n",
              " 'twinkies.jok': 391,\n",
              " 'sanshop.txt': 397,\n",
              " 'scam.txt': 379,\n",
              " 'seeds42.txt': 1245,\n",
              " 'top10st1.txt': 2713,\n",
              " 'top10st2.txt': 4794,\n",
              " 'transp.txt': 261,\n",
              " 'mundane.v2': 1556,\n",
              " 'japantv.txt': 2156,\n",
              " 'johann': 49,\n",
              " 'just2': 171,\n",
              " 'bnb_quot.txt': 986,\n",
              " 'bored.txt': 1325,\n",
              " 'sw_err.txt': 1982,\n",
              " 'skippy.txt': 461,\n",
              " 'recip1.txt': 289,\n",
              " 'shooters.txt': 2307,\n",
              " 'slogans.txt': 130,\n",
              " 'smurf-03.txt': 631,\n",
              " 'smurf_co.txt': 1441,\n",
              " 'soccer.txt': 271,\n",
              " 'beerjesus.hum': 68,\n",
              " 'bozo_tv.leg': 292,\n",
              " 'coffee.txt': 703,\n",
              " 'diet.txt': 199,\n",
              " 'jason.fun': 5784,\n",
              " 'econridl.fun': 641,\n",
              " 'oliver.txt': 1515,\n",
              " 'oliver02.txt': 2194,\n",
              " 'sf-zine.pub': 2842,\n",
              " 'subrdead.hum': 2963,\n",
              " 'topten.hum': 489,\n",
              " 'how2bgod.txt': 503,\n",
              " 'hangover.txt': 118,\n",
              " 'netmask.txt': 401,\n",
              " 'chickens.txt': 266,\n",
              " 'classicm.hum': 1800,\n",
              " 'cmu.share': 553,\n",
              " 'commword.hum': 2725,\n",
              " 'cookbkly.how': 288,\n",
              " 'fearcola.hum': 890,\n",
              " 'hedgehog.txt': 269,\n",
              " 'horflick.txt': 1028,\n",
              " 'alcohol.hum': 223,\n",
              " 'bbh_intv.txt': 1141,\n",
              " 'beer-g': 501,\n",
              " 'beergame.hum': 567,\n",
              " 'dining.out': 550,\n",
              " 'iced.tea': 585,\n",
              " 'thievco.txt': 1701,\n",
              " 'aeonint.txt': 2681,\n",
              " 'mindvox': 4312,\n",
              " 'deathhem.txt': 159,\n",
              " 'bread.rec': 515,\n",
              " 'brownie.rec': 259,\n",
              " 'lion.txt': 900,\n",
              " 'lp-assoc.txt': 524,\n",
              " 'desk.txt': 132,\n",
              " 'disclym.txt': 287,\n",
              " 'flowchrt': 108,\n",
              " 'flux_fix.txt': 1765,\n",
              " 'packard.txt': 1218,\n",
              " 'parsnip.txt': 166,\n",
              " 'penisprt.txt': 159,\n",
              " 'msfields.txt': 107,\n",
              " 'mtv.asc': 155,\n",
              " 'modstup': 352,\n",
              " 'moslem.txt': 141,\n",
              " 'mov_rail.txt': 1378,\n",
              " 'miamimth.txt': 163,\n",
              " 'missdish': 470,\n",
              " 'consp.txt': 4585,\n",
              " 'food': 993,\n",
              " 'foodtips': 767,\n",
              " 'cake.rec': 335,\n",
              " 'docdict.txt': 703,\n",
              " 'drinks.txt': 718,\n",
              " 'drive.txt': 421,\n",
              " 'bhang.fun': 248,\n",
              " 'booze1.fun': 1150,\n",
              " 'booze2.fun': 881,\n",
              " 'cgs_lst.txt': 429,\n",
              " 'golnar.txt': 1323,\n",
              " 'red-neck.jks': 207,\n",
              " 'reddwarf.sng': 143,\n",
              " 'renorthr.txt': 106,\n",
              " 'rns_bcl.txt': 694,\n",
              " 'rns_bwl.txt': 1167,\n",
              " 'rns_ency.txt': 4627,\n",
              " 'robot.tes': 352,\n",
              " 'subb_lis.txt': 1509,\n",
              " 'staff.txt': 192,\n",
              " 'stuf10.txt': 6588,\n",
              " 'stuf11.txt': 4566,\n",
              " 'fusion.gal': 339,\n",
              " 'sfmovie.txt': 1107,\n",
              " 'the_ant.txt': 267,\n",
              " 'mog-history': 1983,\n",
              " 'dalive': 378,\n",
              " 'iremember': 295,\n",
              " 'kilroy': 253,\n",
              " 'cform2.txt': 379,\n",
              " 'msorrow': 3205,\n",
              " 'nameisreo.txt': 1717,\n",
              " 'inquirer.txt': 515,\n",
              " 'ins1': 101,\n",
              " 'episimp2.txt': 2492,\n",
              " 'bnbguide.txt': 3211,\n",
              " 'buldrwho.txt': 616,\n",
              " 'gd_alf.txt': 2196,\n",
              " 'epi_bnb.txt': 3124,\n",
              " 'epi_tton.txt': 5275,\n",
              " 'epi_rns.txt': 2444,\n",
              " 'epi_.txt': 4859,\n",
              " 'epi_merm.txt': 3210,\n",
              " 'gd_gal.txt': 3723,\n",
              " 'twilight.txt': 3230,\n",
              " 'lost.txt': 3799,\n",
              " 'outlimit.txt': 2213,\n",
              " 'scratchy.txt': 3767,\n",
              " 'gd_drwho.txt': 1986,\n",
              " 'gd_frasr.txt': 1420,\n",
              " 'gd_liqtv.txt': 707,\n",
              " 'gd_maxhd.txt': 499,\n",
              " 'gd_hhead.txt': 4407,\n",
              " 'gd_ol.txt': 2213,\n",
              " 'epiquest.txt': 1382,\n",
              " 'gd_tznew.txt': 3245,\n",
              " 'ghostsch.hum': 545,\n",
              " 'gd_sgrnd.txt': 1575,\n",
              " 'gd_ql.txt': 7020,\n",
              " 'a-team': 3431,\n",
              " 'a_fish_c.apo': 178,\n",
              " 'a_tv_t-p.com': 1202,\n",
              " 'aboutada.txt': 379,\n",
              " 'adrian_e.faq': 832,\n",
              " 'allfam.epi': 3027,\n",
              " 'allusion': 2713,\n",
              " 'amazing.epi': 3941,\n",
              " 'anime.cli': 2632,\n",
              " 'anime.lif': 1922,\n",
              " 'hbo_spec.rev': 474,\n",
              " 'highland.epi': 1234,\n",
              " 'hitchcok.txt': 400,\n",
              " 'wacky.ani': 1151,\n",
              " 'wkrp.epi': 2333,\n",
              " 'cast.lis': 817,\n",
              " 'christop.int': 1499,\n",
              " 'chung.iv': 1692,\n",
              " 'clancy.txt': 1057,\n",
              " 'comic_st.gui': 3121,\n",
              " 'cultmov.faq': 5064,\n",
              " 'maxheadr': 744,\n",
              " 'facedeth.txt': 1738,\n",
              " 'farsi.txt': 147,\n",
              " 'nukewar.txt': 3460,\n",
              " 'number': 169,\n",
              " 'spoonlis.txt': 674,\n",
              " 'sinksub.txt': 190,\n",
              " 'sorority.gir': 587,\n",
              " 'petshop': 649,\n",
              " 'prac2.jok': 3691,\n",
              " 'pracjoke.txt': 1418,\n",
              " 'fed.txt': 218,\n",
              " 'cybrtrsh.txt': 3057,\n",
              " 'hstlrtxt.txt': 245,\n",
              " 'homermmm.txt': 2181,\n",
              " 'how2dotv.txt': 272,\n",
              " 'filmgoof.txt': 5340,\n",
              " 'films_gl.txt': 6779,\n",
              " 'fish.rec': 182,\n",
              " 'hitchcoc.app': 434,\n",
              " 'is_story.txt': 516,\n",
              " 'fegg!int.txt': 201,\n",
              " 'feggaqui.txt': 277,\n",
              " 'feggmagi.txt': 308,\n",
              " 'normquot.txt': 1131,\n",
              " 'history2.oop': 692,\n",
              " 'pol-corr.txt': 165,\n",
              " 'poopie.txt': 175,\n",
              " 'ppbeer.txt': 567,\n",
              " 'pot.txt': 1001,\n",
              " 'headlnrs': 296,\n",
              " 'hell.txt': 117,\n",
              " 'hum2': 443,\n",
              " 'humor9.txt': 20288,\n",
              " 'cokeform.txt': 260,\n",
              " 'contract.moo': 908,\n",
              " 'cookberk': 291,\n",
              " 'cookie.1': 2742,\n",
              " 'cooplaws': 222,\n",
              " 'cuisine.txt': 525,\n",
              " 'coffee.faq': 6439,\n",
              " 'modemwld.txt': 939,\n",
              " 'minn.txt': 1680,\n",
              " 'oam-001.txt': 241,\n",
              " 'oam.nfo': 49,\n",
              " 'acronym.txt': 134,\n",
              " 'college.txt': 501,\n",
              " 'english.txt': 919,\n",
              " 'figure_1.txt': 380,\n",
              " 'crzycred.lst': 3137,\n",
              " 'arnold.txt': 1351,\n",
              " 'ateam.epi': 3430,\n",
              " 'avengers.lis': 1458,\n",
              " 'bbc_vide.cat': 1546,\n",
              " 'beauty.tm': 4946,\n",
              " 'blackadd': 1560,\n",
              " 'blake7.lis': 1608,\n",
              " 'doc-says.txt': 227,\n",
              " 'dromes.txt': 1079,\n",
              " 'eatme.txt': 268,\n",
              " 'free-cof.fee': 378,\n",
              " 'initials.rid': 304,\n",
              " 'old.txt': 332,\n",
              " 'post.nuc': 145,\n",
              " 'quantum.phy': 337,\n",
              " 'resolutn.txt': 515,\n",
              " 'soporifi.abs': 269,\n",
              " 'swearfrn.hum': 806,\n",
              " 'tarot.txt': 531,\n",
              " 'c0dez.txt': 5096,\n",
              " 'beer.txt': 904,\n",
              " 'butcher.txt': 1486,\n",
              " 'curiousgeorgie.txt': 325,\n",
              " 'freudonseuss.txt': 315,\n",
              " 'normalboy.txt': 102,\n",
              " 'annoy.fascist': 813,\n",
              " 'booze.fun': 608,\n",
              " 'diesmurf.txt': 248,\n",
              " 'elevator.fun': 323,\n",
              " 'get.drunk.cheap': 1120,\n",
              " 'how.bugs.breakd': 314,\n",
              " 'normal.boy': 103,\n",
              " 'practica.txt': 15284,\n",
              " 'dead2.txt': 2228,\n",
              " 'dead3.txt': 4252,\n",
              " 'dead4.txt': 4901,\n",
              " 'dead5.txt': 5830,\n",
              " 'radexposed.txt': 936,\n",
              " 'carowner.txt': 151,\n",
              " 'pat.txt': 522,\n",
              " 'bugs.txt': 451,\n",
              " 'hack7.txt': 3882,\n",
              " 'happyhack.txt': 886,\n",
              " 'airlines': 268,\n",
              " 'skincat': 831,\n",
              " 'venganza.txt': 211,\n",
              " 'venison.txt': 181,\n",
              " 'homebrew.txt': 2100,\n",
              " 'hop.faq': 3596,\n",
              " 'hotpeper.txt': 67,\n",
              " 'necropls.txt': 676,\n",
              " 'three.txt': 566,\n",
              " 'eggroll1.mea': 128,\n",
              " 'dandwine.bev': 130,\n",
              " 'engmuffn.txt': 149,\n",
              " 'egglentl.vgn': 134,\n",
              " 'egg-bred.txt': 149,\n",
              " 'focaccia.brd': 144,\n",
              " 'feista01.dip': 72,\n",
              " 'frogeye1.sal': 133,\n",
              " 'firecamp.txt': 70,\n",
              " 'goldwatr.txt': 95,\n",
              " 'garlpast.vgn': 122,\n",
              " 'greenchi.txt': 217,\n",
              " 'texbeef.txt': 185,\n",
              " 'unochili.txt': 207,\n",
              " 'wonton.txt': 964,\n",
              " 'woods.txt': 271,\n",
              " 'whitbred.txt': 166,\n",
              " 'gack!.txt': 116,\n",
              " 'yogurt.asc': 214,\n",
              " 'insult.lst': 11706,\n",
              " 'insults1.txt': 7073,\n",
              " 'calamus.hrb': 135,\n",
              " 'damiana.hrb': 153,\n",
              " 'imbecile.txt': 118,\n",
              " 'orgfrost.bev': 56,\n",
              " 'penndtch': 9995,\n",
              " 'pasta001.sal': 104,\n",
              " 'oranchic.pol': 112,\n",
              " 'pbcookie.des': 128,\n",
              " 'jon.txt': 370,\n",
              " 'hackmorality.txt': 6476,\n",
              " 'whatbbs': 385,\n",
              " 'hermsys.txt': 171,\n",
              " 'antimead.bev': 158,\n",
              " 'appbred.brd': 127,\n",
              " 'arcadian.txt': 176,\n",
              " 'apsaucke.des': 138,\n",
              " 'awespinh.sal': 133,\n",
              " 'applepie.des': 90,\n",
              " 'banana05.brd': 56,\n",
              " 'banana01.brd': 67,\n",
              " 'berryeto.bev': 60,\n",
              " 'jawgumbo.fis': 121,\n",
              " 'jalapast.dip': 147,\n",
              " 'batrbred.txt': 115,\n",
              " 'beershrp.fis': 102,\n",
              " 'banana04.brd': 102,\n",
              " 'jawsalad.fis': 109,\n",
              " 'banana03.brd': 169,\n",
              " 'beershrm.fis': 93,\n",
              " 'jambalay.pol': 109,\n",
              " 'banana02.brd': 119,\n",
              " 'baklava.des': 163,\n",
              " 'japice.bev': 55,\n",
              " 'jungjuic.bev': 121,\n",
              " 'montoys.txt': 89,\n",
              " 'seafood.txt': 243,\n",
              " 'zuccmush.sal': 98,\n",
              " 'strattma.txt': 136,\n",
              " 'margos.txt': 200,\n",
              " 'shuimai.txt': 1351,\n",
              " 'stagline.txt': 126,\n",
              " 'mitch.txt': 254,\n",
              " 'zucantom.sal': 123,\n",
              " 'brdpudd.des': 87,\n",
              " 'breadpud.des': 132,\n",
              " 'btscke03.des': 100,\n",
              " 'btscke02.des': 141,\n",
              " 'btscke01.des': 320,\n",
              " 'blkbean.txt': 224,\n",
              " 'boarchil.txt': 282,\n",
              " 'bredcake.des': 109,\n",
              " 'bread.rcp': 5126,\n",
              " 'blkbnsrc.vgn': 154,\n",
              " 'bunacald.fis': 76,\n",
              " 'burrito.mea': 81,\n",
              " 'caramels.des': 74,\n",
              " 'caesardr.sal': 226,\n",
              " 'btscke05.des': 95,\n",
              " 'buffwing.pol': 66,\n",
              " 'capital.txt': 218,\n",
              " 'butstcod.fis': 137,\n",
              " 'btscke04.des': 225,\n",
              " 'advrtize.txt': 560,\n",
              " 'bitnet.txt': 2117,\n",
              " 'nzdrinks.txt': 308,\n",
              " 'recipe.008': 116,\n",
              " 'renored.txt': 194,\n",
              " 'recipe.009': 119,\n",
              " 'recipe.005': 207,\n",
              " 'recipe.006': 135,\n",
              " 'recipe.003': 180,\n",
              " 'recipe.004': 235,\n",
              " 'recipe.012': 104,\n",
              " 'recipe.002': 109,\n",
              " 'recipe.001': 231,\n",
              " 'recipe.007': 183,\n",
              " 'oakwood.txt': 197,\n",
              " 'recipe.010': 158,\n",
              " 'richbred.txt': 165,\n",
              " 'recipe.011': 238,\n",
              " 'oldtime.txt': 464,\n",
              " 'all_grai': 4232,\n",
              " 'amchap2.txt': 105,\n",
              " 'beer.gam': 699,\n",
              " 'beergame.txt': 395,\n",
              " 'beer-gui': 442,\n",
              " 'gotukola.hrb': 775,\n",
              " 'hitler.txt': 438,\n",
              " 'coollngo2.txt': 268,\n",
              " 'jimhood.txt': 803,\n",
              " 'aphrodis.txt': 561,\n",
              " 'antibiot.txt': 1217,\n",
              " 'curry.txt': 460,\n",
              " 'bw.txt': 3738,\n",
              " 'atherosc.txt': 490,\n",
              " 'booknuti.txt': 163,\n",
              " 'chinese.txt': 224,\n",
              " 'ayurved.txt': 410,\n",
              " 'brush1.txt': 1031,\n",
              " 'anorexia.txt': 174,\n",
              " 'back1.txt': 429,\n",
              " 'critic.txt': 2295,\n",
              " 'cereal.txt': 520,\n",
              " 'arthriti.txt': 974,\n",
              " 'beesherb.txt': 435,\n",
              " 'aniherb.txt': 586,\n",
              " 'acetab1.txt': 234,\n",
              " 'blood.txt': 205,\n",
              " '1st_aid.txt': 190,\n",
              " 'acne1.txt': 1030,\n",
              " 'proudlyserve.txt': 1596,\n",
              " 'jayjay.txt': 713,\n",
              " 'bnbeg2.4.txt': 4823,\n",
              " 'att.txt': 411,\n",
              " 'calculus.txt': 215,\n",
              " 'byfb.txt': 3397,\n",
              " 'gumbo.txt': 280,\n",
              " 'btaco.txt': 82,\n",
              " 'beerwarn.txt': 159,\n",
              " 'qttofu.vgn': 79,\n",
              " 'paddingurpapers.txt': 589,\n",
              " 'religion.txt': 666,\n",
              " 'girlspeak.txt': 258,\n",
              " 'enlightenment.txt': 197,\n",
              " 'cartoon_laws.txt': 323,\n",
              " 'llamas.txt': 542,\n",
              " 'ganamembers.txt': 120,\n",
              " 'coffeebeerwomen.txt': 366,\n",
              " 'confucius_say.txt': 299,\n",
              " 'aggie.txt': 129,\n",
              " 'collected_quotes.txt': 4024,\n",
              " 'trekfume.txt': 341,\n",
              " 'labels.txt': 118,\n",
              " 'horoscope.txt': 255,\n",
              " 'fireplacein.txt': 564,\n",
              " 'childrenbooks.txt': 139,\n",
              " 'turing.shr': 360,\n",
              " 'valujet.txt': 1137,\n",
              " 'computer.txt': 639,\n",
              " 'jokes1.txt': 2048,\n",
              " 'lawyers.txt': 173,\n",
              " 'cops.txt': 93,\n",
              " 'fbipizza.txt': 196,\n",
              " 'mcd.txt': 370,\n",
              " 'nasaglenn.txt': 71,\n",
              " 'bhb.ill': 1608,\n",
              " 'bond-2.txt': 317,\n",
              " 'indgrdn.txt': 1982,\n",
              " 'insect1.txt': 260,\n",
              " 'quantity.001': 95,\n",
              " 'sawyer.txt': 812,\n",
              " 'exidy.txt': 526,\n",
              " 'deadlysins.txt': 868,\n",
              " 'yuban.txt': 403,\n",
              " 'planetzero.txt': 584,\n",
              " 'aids.txt': 2340,\n",
              " 'reeves.txt': 1479,\n",
              " 'moore.txt': 671,\n",
              " 'chickenheadbbs.txt': 1884,\n",
              " 'phxbbs-m.txt': 3197,\n",
              " 'ukunderg.txt': 1792,\n",
              " 'silverclaws.txt': 323,\n",
              " 'namaste.txt': 319,\n",
              " 'crazy.txt': 366,\n",
              " 'lifeonledge.txt': 1551,\n",
              " 'lipkovits.txt': 1198,\n",
              " 'basehead.txt': 2968,\n",
              " 'draxamus.txt': 305,\n",
              " 'stressman.txt': 1193,\n",
              " 'exylic.txt': 397,\n",
              " 'apsnet.txt': 350,\n",
              " 'lansing.txt': 514,\n",
              " 'heroic.txt': 146,\n",
              " 'onan.txt': 685,\n",
              " 'teevee.hum': 402,\n",
              " 'disclmr.txt': 287,\n",
              " 'grail.txt': 5322,\n",
              " 'mel.txt': 765,\n",
              " 'hacktest.txt': 1824,\n",
              " 'hackingcracking.txt': 7262,\n",
              " ...}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docu_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYcPElKMOhtn",
        "outputId": "b84b4192-2c43-4c0b-fe8e-4d2c9c0b3431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "matrix=pd.DataFrame()\n",
        "matrix[\"terms\"]=word_list\n",
        "for i in my_list1:\n",
        "  matrix[i]=0\n",
        "matrix.index = list(matrix[\"terms\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUpkl78-1ekd"
      },
      "source": [
        "for raw count type TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcJS6GzXJCTE"
      },
      "outputs": [],
      "source": [
        "for i in word_list:\n",
        "  doc_list=frequency_dict[i]\n",
        "  idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "  for j in my_list1:\n",
        "    if j in frequency_dict[i]:\n",
        "      matrix.loc[i,j]=frequency_dict[i][j]*idf_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "-yuhDqgITFcY",
        "outputId": "de0b12bb-c31d-4782-8adf-1ad16391dfc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 terms  hell.jok  hate.hum  rinaldos.txt  gohome.hum  \\\n",
              "likes            likes       0.0       0.0           0.0         0.0   \n",
              "hysteria      hysteria       0.0       0.0           0.0         0.0   \n",
              "simpletons  simpletons       0.0       0.0           0.0         0.0   \n",
              "wen                wen       0.0       0.0           0.0         0.0   \n",
              "1f                  1f       0.0       0.0           0.0         0.0   \n",
              "...                ...       ...       ...           ...         ...   \n",
              "manning        manning       0.0       0.0           0.0         0.0   \n",
              "spieler        spieler       0.0       0.0           0.0         0.0   \n",
              "danstairs    danstairs       0.0       0.0           0.0         0.0   \n",
              "fechner        fechner       0.0       0.0           0.0         0.0   \n",
              "agonising    agonising       0.0       0.0           0.0         0.0   \n",
              "\n",
              "            harmful.hum  herb!.hum  mothers.txt  grommet.hum  conan.txt  ...  \\\n",
              "likes               0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "hysteria            0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "simpletons          0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "wen                 0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "1f                  0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "...                 ...        ...          ...          ...        ...  ...   \n",
              "manning             0.0        0.0          0.0          0.0   7.824004  ...   \n",
              "spieler             0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "danstairs           0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "fechner             0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "agonising           0.0        0.0          0.0          0.0   0.000000  ...   \n",
              "\n",
              "            freshman.hum  resrch_p.hra  flattax.hum  fwksfun.hum  female.jok  \\\n",
              "likes                0.0           0.0          0.0          0.0         0.0   \n",
              "hysteria             0.0           0.0          0.0          0.0         0.0   \n",
              "simpletons           0.0           0.0          0.0          0.0         0.0   \n",
              "wen                  0.0           0.0          0.0          0.0         0.0   \n",
              "1f                   0.0           0.0          0.0          0.0         0.0   \n",
              "...                  ...           ...          ...          ...         ...   \n",
              "manning              0.0           0.0          0.0          0.0         0.0   \n",
              "spieler              0.0           0.0          0.0          0.0         0.0   \n",
              "danstairs            0.0           0.0          0.0          0.0         0.0   \n",
              "fechner              0.0           0.0          0.0          0.0         0.0   \n",
              "agonising            0.0           0.0          0.0          0.0         0.0   \n",
              "\n",
              "            goforth.hum  men&wome.txt  mensroom.jok  bbq.txt  mowers.txt  \n",
              "likes               0.0           0.0           0.0      0.0         0.0  \n",
              "hysteria            0.0           0.0           0.0      0.0         0.0  \n",
              "simpletons          0.0           0.0           0.0      0.0         0.0  \n",
              "wen                 0.0           0.0           0.0      0.0         0.0  \n",
              "1f                  0.0           0.0           0.0      0.0         0.0  \n",
              "...                 ...           ...           ...      ...         ...  \n",
              "manning             0.0           0.0           0.0      0.0         0.0  \n",
              "spieler             0.0           0.0           0.0      0.0         0.0  \n",
              "danstairs           0.0           0.0           0.0      0.0         0.0  \n",
              "fechner             0.0           0.0           0.0      0.0         0.0  \n",
              "agonising           0.0           0.0           0.0      0.0         0.0  \n",
              "\n",
              "[71092 rows x 1134 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0eba080f-4ce3-4608-b337-22e03570cb5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "      <th>hell.jok</th>\n",
              "      <th>hate.hum</th>\n",
              "      <th>rinaldos.txt</th>\n",
              "      <th>gohome.hum</th>\n",
              "      <th>harmful.hum</th>\n",
              "      <th>herb!.hum</th>\n",
              "      <th>mothers.txt</th>\n",
              "      <th>grommet.hum</th>\n",
              "      <th>conan.txt</th>\n",
              "      <th>...</th>\n",
              "      <th>freshman.hum</th>\n",
              "      <th>resrch_p.hra</th>\n",
              "      <th>flattax.hum</th>\n",
              "      <th>fwksfun.hum</th>\n",
              "      <th>female.jok</th>\n",
              "      <th>goforth.hum</th>\n",
              "      <th>men&amp;wome.txt</th>\n",
              "      <th>mensroom.jok</th>\n",
              "      <th>bbq.txt</th>\n",
              "      <th>mowers.txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>likes</th>\n",
              "      <td>likes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hysteria</th>\n",
              "      <td>hysteria</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simpletons</th>\n",
              "      <td>simpletons</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wen</th>\n",
              "      <td>wen</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1f</th>\n",
              "      <td>1f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>manning</th>\n",
              "      <td>manning</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.824004</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spieler</th>\n",
              "      <td>spieler</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>danstairs</th>\n",
              "      <td>danstairs</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fechner</th>\n",
              "      <td>fechner</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agonising</th>\n",
              "      <td>agonising</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71092 rows  1134 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0eba080f-4ce3-4608-b337-22e03570cb5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0eba080f-4ce3-4608-b337-22e03570cb5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0eba080f-4ce3-4608-b337-22e03570cb5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(listA, listB):\n",
        "  cos_sim = dot(listA, listB) / (norm(listA) * norm(listB))\n",
        "  return cos_sim\n",
        "\n",
        "tfIdf_dict={}\n",
        "str=input(\"enter you query\")\n",
        "str=process(str)\n",
        "dict_temp=collections.Counter(str)\n",
        "list_temp=[]\n",
        "for i in word_list:\n",
        "  if i in dict_temp:\n",
        "    doc_list=frequency_dict[i]\n",
        "    idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "    list_temp.append(dict_temp[i]*idf_value)\n",
        "  else:\n",
        "    list_temp.append(0)\n",
        "for j in my_list1:\n",
        "  doc_vec=matrix[j]\n",
        "  tfIdf_dict[j]=cos_sim(list_temp,doc_vec)\n",
        "sorted_x = sorted(tfIdf_dict.items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkxTNNziTR8p",
        "outputId": "eb1b911b-5ac2-42cf-9d2b-2dd8d4677479"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter you querysweetened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcVlP09ReNhe",
        "outputId": "71e15abe-8d74-4f05-cb7e-7c45ac4d366a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 insect1.txt 0.056426144017128095\n",
            "2 coffee.faq 0.032361738205127354\n",
            "3 coffee.txt 0.025122403633202475\n",
            "4 drinks.txt 0.024313918500031798\n",
            "5 candy.txt 0.0189516533053509\n"
          ]
        }
      ],
      "source": [
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srVE4vio1tBd"
      },
      "source": [
        "for binary type TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRYh0wJG1u6o",
        "outputId": "730e772e-7ec9-45c2-867f-d970129e603b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "matrix2=pd.DataFrame()\n",
        "matrix2[\"terms\"]=word_list\n",
        "for i in my_list1:\n",
        "  matrix2[i]=0\n",
        "matrix2.index = list(matrix2[\"terms\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7MbBq3a19VU"
      },
      "outputs": [],
      "source": [
        "for i in word_list:\n",
        "  temp_list=frequency_dict[i]\n",
        "  idf_value=math.log((1133/(len(temp_list)+1)),2)\n",
        "  for j in my_list1:\n",
        "    if j in frequency_dict[i]:\n",
        "      matrix2.loc[i,j]=(1 if 0 < frequency_dict[i][j] else 0)*idf_value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(listA, listB):\n",
        "  cos_sim = dot(listA, listB) / (norm(listA) * norm(listB))\n",
        "  return cos_sim\n",
        "\n",
        "tfIdf_dict={}\n",
        "str=input(\"enter you query\")\n",
        "str=process(str)\n",
        "dict_temp=collections.Counter(str)\n",
        "list_temp=[]\n",
        "for i in word_list:\n",
        "  if i in dict_temp:\n",
        "    doc_list=frequency_dict[i]\n",
        "    idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "    list_temp.append((1 if 0 < dict_temp[i] else 0)*idf_value)\n",
        "  else:\n",
        "    list_temp.append(0)\n",
        "for j in my_list1:\n",
        "  doc_vec=matrix2[j]\n",
        "  tfIdf_dict[j]=cos_sim(list_temp,doc_vec)\n",
        "sorted_x = sorted(tfIdf_dict.items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xf0iBzJl65Y",
        "outputId": "6fe85cdf-14bf-4d3c-aa5a-f17e9ba46d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter you querysweetened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfPBkSOQ4qAp",
        "outputId": "d2591e2f-2659-4227-cdbb-fe62b1fd018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 bread.rec 0.14233395687250058\n",
            "2 insect1.txt 0.09254189444388729\n",
            "3 booze1.fun 0.08091204352017378\n",
            "4 drinks.txt 0.07641077486291\n",
            "5 coffee.txt 0.06358007251609236\n"
          ]
        }
      ],
      "source": [
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "average method"
      ],
      "metadata": {
        "id": "J3epdTkI-CQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix3=pd.DataFrame()\n",
        "matrix3[\"terms\"]=word_list\n",
        "for i in my_list1:\n",
        "  matrix3[i]=0\n",
        "matrix3.index = list(matrix3[\"terms\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrrDdsyS-di-",
        "outputId": "fd419a37-e630-47d9-bb05-ea43fba41caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_list:\n",
        "  temp_list=frequency_dict[i]\n",
        "  idf_value=math.log((1133/(len(temp_list)+1)),2)\n",
        "  for j in my_list1:\n",
        "    if j in frequency_dict[i]:\n",
        "      matrix3.loc[i,j]=(frequency_dict[i][j]/docu_sum[j])*idf_value"
      ],
      "metadata": {
        "id": "1dVaIjly-oUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(listA, listB):\n",
        "  cos_sim = dot(listA, listB) / (norm(listA) * norm(listB))\n",
        "  return cos_sim\n",
        "\n",
        "tfIdf_dict={}\n",
        "str=input(\"enter you query\")\n",
        "str=process(str)\n",
        "dict_temp=collections.Counter(str)\n",
        "sum=sum(dict_temp.values())\n",
        "list_temp=[]\n",
        "for i in word_list:\n",
        "  if i in dict_temp:\n",
        "    doc_list=frequency_dict[i]\n",
        "    idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "    list_temp.append((dict_temp[i]/sum)*idf_value)\n",
        "  else:\n",
        "    list_temp.append(0)\n",
        "for j in my_list1:\n",
        "  doc_vec=matrix3[j]\n",
        "  tfIdf_dict[j]=cos_sim(list_temp,doc_vec)\n",
        "sorted_x = sorted(tfIdf_dict.items(), key=operator.itemgetter(1),reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPfOIurHnMZ9",
        "outputId": "e1e265ab-16ca-4015-d6d0-bc02921e13ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter you querysweetened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpQupSGE_ABB",
        "outputId": "349e1c2d-38b0-4efe-e408-fc7702575347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 insect1.txt 0.0564261440171281\n",
            "2 coffee.faq 0.03236173820512736\n",
            "3 coffee.txt 0.025122403633202475\n",
            "4 drinks.txt 0.0243139185000318\n",
            "5 candy.txt 0.0189516533053509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for type 4 - log normalization"
      ],
      "metadata": {
        "id": "4oQ6pAf-AwXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix4=pd.DataFrame()\n",
        "matrix4[\"terms\"]=word_list\n",
        "for i in my_list1:\n",
        "  matrix4[i]=0\n",
        "matrix4.index = list(matrix4[\"terms\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjSCtF-DBMP4",
        "outputId": "5029e102-28ad-42b4-c606-c8f7a1d04fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_list:\n",
        "  temp_list=frequency_dict[i]\n",
        "  idf_value=math.log((1133/(len(temp_list)+1)),2)\n",
        "  for j in my_list1:\n",
        "    if j in frequency_dict[i]:\n",
        "      matrix4.loc[i,j]=math.log((1+frequency_dict[i][j]),2)*idf_value"
      ],
      "metadata": {
        "id": "UXXWinysBb1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(listA, listB):\n",
        "  cos_sim = dot(listA, listB) / (norm(listA) * norm(listB))\n",
        "  return cos_sim\n",
        "\n",
        "tfIdf_dict={}\n",
        "str=input(\"enter you query\")\n",
        "str=process(str)\n",
        "dict_temp=collections.Counter(str)\n",
        "list_temp=[]\n",
        "for i in word_list:\n",
        "  if i in dict_temp:\n",
        "    doc_list=frequency_dict[i]\n",
        "    idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "    list_temp.append((math.log((1+dict_temp[i]),2))*idf_value)\n",
        "  else:\n",
        "    list_temp.append(0)\n",
        "for j in my_list1:\n",
        "  doc_vec=matrix4[j]\n",
        "  tfIdf_dict[j]=cos_sim(list_temp,doc_vec)\n",
        "sorted_x = sorted(tfIdf_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7P-_dG5CVuL",
        "outputId": "d8e81bdb-2e2c-49a5-f84f-78098d039058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter you querysweetened\n",
            "1 insect1.txt 0.07795707554642786\n",
            "2 bread.rec 0.059714288947216386\n",
            "3 drinks.txt 0.054956932896472\n",
            "4 coffee.txt 0.0504734535605055\n",
            "5 coffee.faq 0.04715507493386254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for double normalization"
      ],
      "metadata": {
        "id": "YfH1n79aCfJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix5=pd.DataFrame()\n",
        "matrix5[\"terms\"]=word_list\n",
        "for i in my_list1:\n",
        "  matrix5[i]=0\n",
        "matrix5.index = list(matrix5[\"terms\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNcxNNgTCiwW",
        "outputId": "404f902b-4039-4e5d-daff-0792413ce09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_list:\n",
        "  temp_list=frequency_dict[i]\n",
        "  idf_value=math.log((1133/(len(temp_list)+1)),2)\n",
        "  for j in my_list1:\n",
        "    if j in frequency_dict[i]:\n",
        "      matrix5.loc[i,j]=(0.5+0.5*(frequency_dict[i][j]/max(docu_list[j])))*idf_value"
      ],
      "metadata": {
        "id": "feOOxi7VC5zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import operator\n",
        "\n",
        "max_dict = {'Australia':178, 'Germany':213, 'Japan': 867}\n",
        "sorted_x = sorted(max_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
        "print((sorted_x[0][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPZCPcBzwB9K",
        "outputId": "4bf2128c-1cfd-4b4c-c028-f913f5761297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(listA, listB):\n",
        "  cos_sim = dot(listA, listB) / (norm(listA) * norm(listB))\n",
        "  return cos_sim\n",
        "\n",
        "tfIdf_dict={}\n",
        "str=input(\"enter you query\")\n",
        "str=process(str)\n",
        "dict_temp=collections.Counter(str)\n",
        "sorted_x_temp = sorted(max_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
        "max=sorted_x_temp[0][1]\n",
        "list_temp=[]\n",
        "for i in word_list:\n",
        "  if i in dict_temp:\n",
        "    doc_list=frequency_dict[i]\n",
        "    idf_value=math.log((1133/(len(doc_list)+1)),2)\n",
        "    list_temp.append((0.5+0.5*(dict_temp[i]/max))*idf_value)\n",
        "  else:\n",
        "    list_temp.append(0)\n",
        "for j in my_list1:\n",
        "  doc_vec=matrix5[j]\n",
        "  tfIdf_dict[j]=cos_sim(list_temp,doc_vec)\n",
        "sorted_x = sorted(tfIdf_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
        "j=1\n",
        "for i in sorted_x:\n",
        "  if(j>5):\n",
        "    break\n",
        "  print(j,i[0],i[1]) \n",
        "  j+=1\n",
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBNG0J2CDyUo",
        "outputId": "baa11e82-016a-4650-cc1a-2a50c9f09016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter you querysweetened\n",
            "1 bread.rec 0.13498457478768267\n",
            "2 insect1.txt 0.09044351897667241\n",
            "3 booze1.fun 0.0791857573030117\n",
            "4 drinks.txt 0.07537148890479733\n",
            "5 coffee.txt 0.06238572523503957\n",
            "1 bread.rec 0.13498457478768267\n",
            "2 insect1.txt 0.09044351897667241\n",
            "3 booze1.fun 0.0791857573030117\n",
            "4 drinks.txt 0.07537148890479733\n",
            "5 coffee.txt 0.06238572523503957\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Solution_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}